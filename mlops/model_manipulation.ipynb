{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### First we perform a data preperation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "def pad_riders(rider_list, max_riders, pad_value='Unknown'):\n",
    "        if len(rider_list) < max_riders:\n",
    "            return rider_list + [pad_value] * (max_riders - len(rider_list))\n",
    "        else:\n",
    "            return rider_list[:max_riders]\n",
    "\n",
    "def preprocess_data(index):\n",
    "    merged_data = pd.read_csv('../common/final_data.csv')\n",
    "\n",
    "    # Define feature groups\n",
    "    race_numerical = [\n",
    "        'distance', 'vertical_meters', 'speed', 'year', 'score', 'quality', 'ranking'\n",
    "    ]\n",
    "    race_categorical = ['name']\n",
    "    rider_numerical = [\n",
    "        'weight', 'height', 'one_day', 'gc', 'tt', 'sprint',\n",
    "        'climber', 'hills', 'age'\n",
    "    ]\n",
    "    rider_categorical_low = ['speciality']\n",
    "    rider_categorical_high = ['nationality', 'team', 'rider_name']\n",
    "\n",
    "    # Split data into training and testing sets based on 'year'\n",
    "    train_data = merged_data[merged_data['year'] < 2024]\n",
    "    test_data = merged_data[merged_data['year'] == 2024]\n",
    "\n",
    "    # Add data from test set to training set incrementally\n",
    "    unique_race_names = test_data['name'].unique()\n",
    "    races_to_move = unique_race_names[:index+1]\n",
    "\n",
    "    race_data_to_move = test_data[test_data['name'].isin(races_to_move)]\n",
    "    train_data = pd.concat([train_data, race_data_to_move], ignore_index=True)\n",
    "    test_data = test_data[~test_data['name'].isin(races_to_move)].reset_index(drop=True)\n",
    "\n",
    "    # Determine the maximum number of riders across all races in both training and testing data\n",
    "    max_riders_train = train_data.groupby(['name', 'year']).size().max()\n",
    "    max_riders_test = test_data.groupby(['name', 'year']).size().max()\n",
    "    max_riders = max(max_riders_train, max_riders_test)\n",
    "    print(f\"Maximum number of riders: {max_riders}\")\n",
    "\n",
    "    # Create preprocessing pipelines\n",
    "    race_numeric_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='mean')),\n",
    "        ('scaler', MinMaxScaler())\n",
    "    ])\n",
    "\n",
    "    race_categorical_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='constant', fill_value='Unknown')),\n",
    "        ('onehot', OneHotEncoder(\n",
    "            drop='first', sparse_output=False, handle_unknown='ignore'\n",
    "        ))\n",
    "    ])\n",
    "\n",
    "    rider_numeric_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='mean')),\n",
    "        ('scaler', MinMaxScaler())\n",
    "    ])\n",
    "\n",
    "    rider_categorical_low_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(\n",
    "            strategy='constant', fill_value='Unknown')),\n",
    "        ('onehot', OneHotEncoder(\n",
    "            drop='first', sparse_output=False, handle_unknown='ignore'\n",
    "        ))\n",
    "    ])\n",
    "\n",
    "    rider_categorical_high_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(\n",
    "            strategy='constant', fill_value='Unknown')),\n",
    "        ('ordinal', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1))\n",
    "    ])\n",
    "\n",
    "    # Fit pipelines on training data\n",
    "    race_numeric_pipeline.fit(train_data[race_numerical])\n",
    "    race_categorical_pipeline.fit(train_data[race_categorical])\n",
    "    rider_numeric_pipeline.fit(train_data[rider_numerical])\n",
    "    rider_categorical_low_pipeline.fit(train_data[rider_categorical_low])\n",
    "    rider_categorical_high_pipeline.fit(train_data[rider_categorical_high])\n",
    "\n",
    "    # Initialize lists for training data\n",
    "    races_train = []\n",
    "    targets_train = []\n",
    "    rider_names_train = []\n",
    "\n",
    "    for (race_name, year), group in train_data.groupby(['name', 'year']):\n",
    "        try:\n",
    "            # Extract race-level features\n",
    "            race_num_data = group[race_numerical].iloc[[0]]\n",
    "            race_cat_data = group[race_categorical].iloc[[0]]\n",
    "\n",
    "            # Extract rider-level features\n",
    "            rider_num_data = group[rider_numerical]\n",
    "            rider_cat_data_low = group[rider_categorical_low]\n",
    "            rider_cat_data_high = group[rider_categorical_high]\n",
    "\n",
    "            # Transform features using fitted pipelines\n",
    "            race_num_processed = race_numeric_pipeline.transform(race_num_data)\n",
    "            race_cat_processed = race_categorical_pipeline.transform(race_cat_data)\n",
    "            rider_num_processed = rider_numeric_pipeline.transform(rider_num_data)\n",
    "            rider_cat_low_processed = rider_categorical_low_pipeline.transform(rider_cat_data_low)\n",
    "            rider_cat_high_processed = rider_categorical_high_pipeline.transform(rider_cat_data_high)\n",
    "\n",
    "            # Combine features\n",
    "            race_features = np.hstack((race_num_processed, race_cat_processed))\n",
    "            rider_features = np.hstack((rider_num_processed, rider_cat_low_processed, rider_cat_high_processed))\n",
    "\n",
    "            # Pad or truncate rider_features to max_riders\n",
    "            n_riders = rider_features.shape[0]\n",
    "            if n_riders < max_riders:\n",
    "                pad_width = max_riders - n_riders\n",
    "                padded_rider_features = np.pad(\n",
    "                    rider_features,\n",
    "                    ((0, pad_width), (0, 0)),\n",
    "                    mode='constant',\n",
    "                    constant_values=0\n",
    "                )\n",
    "            else:\n",
    "                padded_rider_features = rider_features[:max_riders, :]\n",
    "\n",
    "            # Create feature matrix by repeating race_features and concatenating with rider_features\n",
    "            feature_matrix = np.hstack((\n",
    "                np.tile(race_features, (max_riders, 1)),\n",
    "                padded_rider_features\n",
    "            ))\n",
    "\n",
    "            # Calculate probabilities for first 3 riders and pad or truncate to max_riders\n",
    "            ranks = group['rank'].values\n",
    "            padded_probabilities = np.zeros(max_riders)\n",
    "            probabilities = np.array([np.exp(-ranks[:3]) / np.sum(np.exp(-ranks[:3]))])\n",
    "            padded_probabilities[0:3] = probabilities\n",
    "\n",
    "            # Collect rider names and pad or truncate to max_riders\n",
    "            riders = group['rider_name'].tolist()\n",
    "            if n_riders < max_riders:\n",
    "                padded_riders = riders + ['PAD'] * (max_riders - n_riders)\n",
    "            else:\n",
    "                padded_riders = riders[:max_riders]\n",
    "\n",
    "            # Append data to lists\n",
    "            races_train.append(feature_matrix)\n",
    "            targets_train.append(padded_probabilities)\n",
    "            rider_names_train.append(padded_riders)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing race {race_name} {year}: {e}\")\n",
    "            continue\n",
    "\n",
    "    # Initialize lists for test data\n",
    "    races_test = []\n",
    "    targets_test = []\n",
    "    rider_names_test = []\n",
    "\n",
    "    for (race_name, year), group in test_data.groupby(['name', 'year']):\n",
    "        try:\n",
    "            # Extract race-level features\n",
    "            race_num_data = group[race_numerical].iloc[[0]]\n",
    "            race_cat_data = group[race_categorical].iloc[[0]]\n",
    "\n",
    "            # Extract rider-level features\n",
    "            rider_num_data = group[rider_numerical]\n",
    "            rider_cat_data_low = group[rider_categorical_low]\n",
    "            rider_cat_data_high = group[rider_categorical_high]\n",
    "\n",
    "            # Transform features using pipelines fitted on training data\n",
    "            race_num_processed = race_numeric_pipeline.transform(race_num_data)\n",
    "            race_cat_processed = race_categorical_pipeline.transform(race_cat_data)\n",
    "            rider_num_processed = rider_numeric_pipeline.transform(rider_num_data)\n",
    "            rider_cat_low_processed = rider_categorical_low_pipeline.transform(rider_cat_data_low)\n",
    "            rider_cat_high_processed = rider_categorical_high_pipeline.transform(rider_cat_data_high)\n",
    "\n",
    "            # Combine features\n",
    "            race_features = np.hstack((race_num_processed, race_cat_processed))\n",
    "            rider_features = np.hstack((rider_num_processed, rider_cat_low_processed, rider_cat_high_processed))\n",
    "\n",
    "            # Pad or truncate rider_features to max_riders\n",
    "            n_riders = rider_features.shape[0]\n",
    "            if n_riders < max_riders:\n",
    "                pad_width = max_riders - n_riders\n",
    "                padded_rider_features = np.pad(\n",
    "                    rider_features,\n",
    "                    ((0, pad_width), (0, 0)),\n",
    "                    mode='constant',\n",
    "                    constant_values=0\n",
    "                )\n",
    "            else:\n",
    "                padded_rider_features = rider_features[:max_riders, :]\n",
    "\n",
    "            # Create feature matrix by repeating race_features and concatenating with rider_features\n",
    "            feature_matrix = np.hstack((\n",
    "                np.tile(race_features, (max_riders, 1)),\n",
    "                padded_rider_features\n",
    "            ))\n",
    "\n",
    "            # Calculate probabilities for first 3 riders and pad or truncate to max_riders\n",
    "            ranks = group['rank'].values\n",
    "            padded_probabilities = np.zeros(max_riders)\n",
    "            probabilities = np.array([np.exp(-ranks[:3]) / np.sum(np.exp(-ranks[:3]))])\n",
    "            padded_probabilities[0:3] = probabilities\n",
    "\n",
    "            # Collect rider names and pad or truncate to max_riders\n",
    "            riders = group['rider_name'].tolist()\n",
    "            if n_riders < max_riders:\n",
    "                padded_riders = riders + ['PAD'] * (max_riders - n_riders)\n",
    "            else:\n",
    "                padded_riders = riders[:max_riders]\n",
    "\n",
    "            # Append data to lists\n",
    "            races_test.append(feature_matrix)\n",
    "            targets_test.append(padded_probabilities)\n",
    "            rider_names_test.append(padded_riders)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing race {race_name} {year}: {e}\")\n",
    "            continue\n",
    "    # Find maximum number of riders across all data\n",
    "    max_riders = max(\n",
    "        max(len(riders) for riders in rider_names_train),\n",
    "        max(len(riders) for riders in rider_names_test)\n",
    "    )\n",
    "\n",
    "    # Convert lists to NumPy arrays\n",
    "    X_train = np.array(races_train)\n",
    "    y_train = np.array(targets_train)\n",
    "    rider_names_train = np.array(rider_names_train, dtype=object)\n",
    "\n",
    "    X_test = np.array(races_test)\n",
    "    y_test = np.array(targets_test)\n",
    "    rider_names_test = np.array(rider_names_test, dtype=object)\n",
    "\n",
    "    # Save the data\n",
    "    np.save('X_train.npy', X_train)\n",
    "    np.save('y_train.npy', y_train)\n",
    "    np.save('rider_names_train.npy', rider_names_train)\n",
    "\n",
    "    np.save('X_test.npy', X_test)\n",
    "    np.save('y_test.npy', y_test)\n",
    "    np.save('rider_names_test.npy', rider_names_test)\n",
    "\n",
    "    print(\"Data preprocessing completed and saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set Mlflow tracking uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "# Set MLflow experiment\n",
    "mlflow.set_tracking_uri(\"http://seito.lavbic.net:5000\")\n",
    "mlflow.set_experiment(\"Race_Prediction_Experiment_I\")\n",
    "client = MlflowClient()\n",
    "\n",
    "# get the best model id\n",
    "experiment_name = \"Race_Prediction_Experiment_I\"\n",
    "experiment_id = client.get_experiment_by_name(experiment_name).experiment_id\n",
    "runs = client.search_runs(experiment_id, order_by=[\"metrics.test_mae ASC\"], max_results=1)\n",
    "best_run = runs[0]\n",
    "\n",
    "best_params = best_run.data.params\n",
    "print(\"Best Run Parameters:\")\n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrain the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "from mlflow.models.signature import infer_signature\n",
    "\n",
    "# Define the model class\n",
    "class RaceRegressionModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=128):\n",
    "        super(RaceRegressionModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out.squeeze()\n",
    "\n",
    "# Define the dataset class\n",
    "class RaceRegressionDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "    \n",
    "def retrain():\n",
    "    # Load the data (adjust file paths as needed)\n",
    "    X_train = np.load('X_train.npy', allow_pickle=True)\n",
    "    y_train = np.load('y_train.npy', allow_pickle=True)\n",
    "    X_test = np.load('X_test.npy', allow_pickle=True)\n",
    "    y_test = np.load('y_test.npy', allow_pickle=True)\n",
    "\n",
    "    # Flatten the data for scikit-learn models\n",
    "    X_train_flat = X_train.reshape(-1, X_train.shape[2])    # Shape: (num_races_train * max_riders, num_features)\n",
    "    X_test_flat = X_test.reshape(-1, X_test.shape[2])       # Shape: (num_races_test * max_riders, num_features)\n",
    "\n",
    "    # Flatten the targets\n",
    "    y_train_flat = y_train.flatten()  # Shape: (num_races_train * max_riders,)\n",
    "    y_test_flat = y_test.flatten()    # Shape: (num_races_test * max_riders,)\n",
    "\n",
    "    # Create datasets\n",
    "    train_dataset = RaceRegressionDataset(X_train_flat, y_train_flat)\n",
    "    test_dataset = RaceRegressionDataset(X_test_flat, y_test_flat)\n",
    "\n",
    "    # Create data loaders with the best batch size\n",
    "    train_loader = DataLoader(train_dataset, batch_size=int(best_params['batch_size']), shuffle=True, num_workers=0)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=int(best_params['batch_size']), shuffle=False, num_workers=0)\n",
    "\n",
    "    # Initialize the model, optimizer, and loss function\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    input_size = X_train_flat.shape[1]\n",
    "\n",
    "    model = RaceRegressionModel(input_size, int(best_params['hidden_size'])).to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=float(best_params['learning_rate']), weight_decay=float(best_params['weight_decay']))\n",
    "\n",
    "    # Start MLflow run\n",
    "    with mlflow.start_run(run_name=\"Retrained Best Model\"):\n",
    "        # Log parameters\n",
    "        mlflow.log_params(best_params)\n",
    "\n",
    "        # Training loop\n",
    "        num_epochs = best_params['num_epochs']\n",
    "        for epoch in range(int(num_epochs)):\n",
    "            model.train()\n",
    "            total_loss = 0\n",
    "            for X_batch, y_batch in train_loader:\n",
    "                X_batch = X_batch.to(device)\n",
    "                y_batch = y_batch.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(X_batch)\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                total_loss += loss.item() * X_batch.size(0)\n",
    "\n",
    "            average_loss = total_loss / len(train_loader.dataset)\n",
    "            mlflow.log_metric(\"train_loss\", average_loss, step=epoch)\n",
    "            print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {average_loss:.4f}\")\n",
    "\n",
    "        # Evaluation on test set\n",
    "        model.eval()\n",
    "        y_true_list = []\n",
    "        y_pred_list = []\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in test_loader:\n",
    "                X_batch = X_batch.to(device)\n",
    "                y_batch = y_batch.to(device)\n",
    "\n",
    "                outputs = model(X_batch)\n",
    "                y_true_list.extend(y_batch.cpu().numpy())\n",
    "                y_pred_list.extend(outputs.cpu().numpy())\n",
    "\n",
    "        y_true_array = np.array(y_true_list)\n",
    "        y_pred_array = np.array(y_pred_list)\n",
    "\n",
    "        # Compute evaluation metrics\n",
    "        mse = mean_squared_error(y_true_array, y_pred_array)\n",
    "        mae = mean_absolute_error(y_true_array, y_pred_array)\n",
    "        rmse = np.sqrt(mse)\n",
    "        r2 = r2_score(y_true_array, y_pred_array)\n",
    "\n",
    "        def mean_absolute_percentage_error(y_true, y_pred):\n",
    "            epsilon = 1e-8  # Avoid division by zero\n",
    "            mask = np.abs(y_true) > epsilon\n",
    "            return np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100\n",
    "\n",
    "        def symmetric_mean_absolute_percentage_error(y_true, y_pred):\n",
    "            denominator = (np.abs(y_true) + np.abs(y_pred)) / 2\n",
    "            diff = np.abs(y_pred - y_true)\n",
    "            mask = denominator != 0\n",
    "            return np.mean((diff[mask] / denominator[mask])) * 100\n",
    "\n",
    "        mape = mean_absolute_percentage_error(y_true_array, y_pred_array)\n",
    "        smape = symmetric_mean_absolute_percentage_error(y_true_array, y_pred_array)\n",
    "\n",
    "        # Log metrics\n",
    "        mlflow.log_metrics({\n",
    "            'test_mse': mse,\n",
    "            'test_mae': mae,\n",
    "            'test_rmse': rmse,\n",
    "            'test_r2': r2,\n",
    "            'test_mape': mape,\n",
    "            'test_smape': smape\n",
    "        })\n",
    "\n",
    "        # Log the model\n",
    "        input_example = X_train_flat[:5].astype(np.float32)\n",
    "        input_example_tensor = torch.tensor(input_example, dtype=torch.float32).to(device)\n",
    "        signature = infer_signature(\n",
    "            input_example,\n",
    "            model(input_example_tensor).cpu().detach().numpy()\n",
    "        )\n",
    "        mlflow.pytorch.log_model(\n",
    "            pytorch_model=model,\n",
    "            artifact_path=\"model\",\n",
    "            input_example=input_example,\n",
    "            signature=signature\n",
    "        )\n",
    "        run = mlflow.active_run()\n",
    "\n",
    "    print(\"Training complete. Model and metrics logged to MLflow.\")\n",
    "    return run.info.run_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Promote model to production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "def deploy_and_overwrite_model(run_id):\n",
    "    client = MlflowClient()\n",
    "    model_name = \"Race prediction\"\n",
    "\n",
    "    # Register the new model version\n",
    "    model_uri = f\"runs:/{run_id}/model\"\n",
    "    new_model_version = client.create_model_version(\n",
    "        name=model_name,\n",
    "        source=model_uri,\n",
    "        run_id=run_id\n",
    "    )\n",
    "\n",
    "    # Transition the new model version to \"Production\"\n",
    "    client.transition_model_version_stage(\n",
    "        name=model_name,\n",
    "        version=new_model_version.version,\n",
    "        stage=\"Production\"\n",
    "    )\n",
    "    print(f\"New model version {new_model_version.version} deployed to production.\")\n",
    "\n",
    "    # Archive old production model versions\n",
    "    for mv in client.search_model_versions(f\"name='{model_name}'\"):\n",
    "        if mv.current_stage == \"Production\" and mv.version != str(new_model_version.version):\n",
    "            client.transition_model_version_stage(\n",
    "                name=model_name,\n",
    "                version=mv.version,\n",
    "                stage=\"Archived\"\n",
    "            )\n",
    "            print(f\"Archived previous model version {mv.version}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testiraj redeployment modela, ce vse dela je to zmaga"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Poglej se mal ci/cd prakse za devops in tut mlops"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
