{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1183593d0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import mlflow.pytorch\n",
    "from mlflow.models.signature import infer_signature\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import optuna\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (2034, 207, 227)\n",
      "y_train shape: (2034, 207)\n",
      "X_test shape: (153, 207, 227)\n",
      "y_test shape: (153, 207)\n"
     ]
    }
   ],
   "source": [
    "# Load the data (adjust file paths as needed)\n",
    "X_train = np.load('X_train.npy', allow_pickle=True)\n",
    "y_train = np.load('y_train.npy', allow_pickle=True)\n",
    "X_test = np.load('X_test.npy', allow_pickle=True)\n",
    "y_test = np.load('y_test.npy', allow_pickle=True)\n",
    "\n",
    "print(f'X_train shape: {X_train.shape}')  # Expected: (num_races_train, max_riders, num_features)\n",
    "print(f'y_train shape: {y_train.shape}')  # Expected: (num_races_train, max_riders)\n",
    "print(f'X_test shape: {X_test.shape}')    # Expected: (num_races_test, max_riders, num_features)\n",
    "print(f'y_test shape: {y_test.shape}')    # Expected: (num_races_test, max_riders)\n",
    "\n",
    "# Flatten the data for scikit-learn models\n",
    "X_train_flat = X_train.reshape(-1, X_train.shape[2])    # Shape: (num_races_train * max_riders, num_features)\n",
    "X_test_flat = X_test.reshape(-1, X_test.shape[2])       # Shape: (num_races_test * max_riders, num_features)\n",
    "\n",
    "# Flatten the targets\n",
    "y_train_flat = y_train.flatten()  # Shape: (num_races_train * max_riders,)\n",
    "y_test_flat = y_test.flatten()    # Shape: (num_races_test * max_riders,)\n",
    "\n",
    "# Filter out invalid targets (if necessary)\n",
    "valid_indices_train = y_train_flat > 0\n",
    "valid_indices_test = y_test_flat > 0\n",
    "\n",
    "X_train_flat = X_train_flat[valid_indices_train]\n",
    "y_train_flat = y_train_flat[valid_indices_train]\n",
    "\n",
    "X_test_flat = X_test_flat[valid_indices_test]\n",
    "y_test_flat = y_test_flat[valid_indices_test]\n",
    "\n",
    "# # Optionally scale the features\n",
    "# scaler = StandardScaler()\n",
    "# X_train_flat = scaler.fit_transform(X_train_flat)\n",
    "# X_test_flat = scaler.transform(X_test_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/11/30 14:07:07 INFO mlflow.tracking.fluent: Experiment with name 'Race_Prediction_Experiment_II' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='file:///Users/feliks/Documents/Faks/Diplomska/App/mlruns/586264776644289656', creation_time=1732972027382, experiment_id='586264776644289656', last_update_time=1732972027382, lifecycle_stage='active', name='Race_Prediction_Experiment_II', tags={}>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set MLflow experiment\n",
    "mlflow.set_tracking_uri(\"file:./mlruns\")\n",
    "mlflow.set_experiment(\"Race_Prediction_Experiment_II\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_model(model_class, param_grid, model_name, X_train, y_train, X_test, y_test):\n",
    "    from itertools import product\n",
    "    import pandas as pd\n",
    "\n",
    "    # Generate all combinations of hyperparameters\n",
    "    keys = param_grid.keys()\n",
    "    values = (param_grid[key] for key in keys)\n",
    "    param_combinations = [dict(zip(keys, combination)) for combination in product(*values)]\n",
    "\n",
    "    # For each combination, train and log the model\n",
    "    for idx, params in enumerate(param_combinations):\n",
    "        # Initialize model with current hyperparameters\n",
    "        model = model_class(**params)\n",
    "\n",
    "        # Fit the model\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Predict on test set\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        # Calculate metrics\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        mape = mean_absolute_percentage_error(y_test, y_pred)\n",
    "        rmse = np.sqrt(mse)\n",
    "        smape = symmetric_mean_absolute_percentage_error(y_test, y_pred)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "        # Start MLflow run\n",
    "        with mlflow.start_run(run_name=f\"{model_name} - Run {idx+1}\"):\n",
    "            # Log parameters\n",
    "            mlflow.log_param(\"model_class\", model_name)\n",
    "            mlflow.log_params(params)\n",
    "\n",
    "            # Log metrics\n",
    "            mlflow.log_metric(\"test_mse\", mse)\n",
    "            mlflow.log_metric(\"test_mae\", mae)\n",
    "            mlflow.log_metric(\"test_r2\", r2)\n",
    "            mlflow.log_metric(\"test_mape\", mape)\n",
    "            mlflow.log_metric(\"test_rmse\", rmse)\n",
    "            mlflow.log_metric(\"test_smape\", smape)\n",
    "\n",
    "            # Log the model\n",
    "            input_example = X_train[:5]\n",
    "            signature = infer_signature(X_train, model.predict(X_train))\n",
    "\n",
    "            mlflow.sklearn.log_model(\n",
    "                sk_model=model,\n",
    "                artifact_path=\"model\",\n",
    "                input_example=input_example,\n",
    "                signature=signature\n",
    "            )\n",
    "\n",
    "            # Print results\n",
    "            print(f\"\\n{model_name} Run {idx+1} parameters: {params}\")\n",
    "            print(f\"{model_name} Run {idx+1} Test MSE: {mse:.4f}\")\n",
    "            print(f\"{model_name} Run {idx+1} Test MAE: {mae:.4f}\")\n",
    "            print(f\"{model_name} Run {idx+1} Test R^2 Score: {r2:.4f}\")\n",
    "            print(f\"{model_name} Run {idx+1} Test MAPE: {mape:.4f}\")\n",
    "            print(f\"{model_name} Run {idx+1} Test RMSE: {rmse:.4f}\")\n",
    "            print(f\"{model_name} Run {idx+1} Test sMAPE: {smape:.4f}\")\n",
    "\n",
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    epsilon = 1e-8  # Small number to prevent division by zero\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    # Avoid division by zero\n",
    "    mask = np.abs(y_true) > epsilon\n",
    "    if np.sum(mask) == 0:\n",
    "        return np.inf  # Return infinity if no valid entries\n",
    "    mape = np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100\n",
    "    return mape\n",
    "\n",
    "def symmetric_mean_absolute_percentage_error(y_true, y_pred):\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    denominator = (np.abs(y_true) + np.abs(y_pred)) / 2\n",
    "    diff = np.abs(y_pred - y_true)\n",
    "    # Avoid division by zero\n",
    "    mask = denominator != 0\n",
    "    smape = np.mean((diff[mask] / denominator[mask])) * 100\n",
    "    return smape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Linear Regression Run 1 parameters: {}\n",
      "Linear Regression Run 1 Test MSE: 0.0560\n",
      "Linear Regression Run 1 Test MAE: 0.2105\n",
      "Linear Regression Run 1 Test R^2 Score: 0.0456\n",
      "Linear Regression Run 1 Test MAPE: 112.0206\n",
      "Linear Regression Run 1 Test RMSE: 0.2367\n",
      "Linear Regression Run 1 Test sMAPE: 67.6160\n"
     ]
    }
   ],
   "source": [
    "linear_reg = LinearRegression\n",
    "param_grid_lr = {\n",
    "    # No hyperparameters to tune\n",
    "}\n",
    "\n",
    "train_and_evaluate_model(\n",
    "    model_class=linear_reg,\n",
    "    param_grid=param_grid_lr,\n",
    "    model_name=\"Linear Regression\",\n",
    "    X_train=X_train_flat,\n",
    "    y_train=y_train_flat,\n",
    "    X_test=X_test_flat,\n",
    "    y_test=y_test_flat\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ridge Regression Run 1 parameters: {'alpha': 0.1}\n",
      "Ridge Regression Run 1 Test MSE: 0.0560\n",
      "Ridge Regression Run 1 Test MAE: 0.2105\n",
      "Ridge Regression Run 1 Test R^2 Score: 0.0456\n",
      "Ridge Regression Run 1 Test MAPE: 112.0234\n",
      "Ridge Regression Run 1 Test RMSE: 0.2367\n",
      "Ridge Regression Run 1 Test sMAPE: 67.6194\n",
      "\n",
      "Ridge Regression Run 2 parameters: {'alpha': 0.9}\n",
      "Ridge Regression Run 2 Test MSE: 0.0560\n",
      "Ridge Regression Run 2 Test MAE: 0.2105\n",
      "Ridge Regression Run 2 Test R^2 Score: 0.0453\n",
      "Ridge Regression Run 2 Test MAPE: 112.0267\n",
      "Ridge Regression Run 2 Test RMSE: 0.2367\n",
      "Ridge Regression Run 2 Test sMAPE: 67.6403\n",
      "\n",
      "Ridge Regression Run 3 parameters: {'alpha': 1.0}\n",
      "Ridge Regression Run 3 Test MSE: 0.0560\n",
      "Ridge Regression Run 3 Test MAE: 0.2105\n",
      "Ridge Regression Run 3 Test R^2 Score: 0.0453\n",
      "Ridge Regression Run 3 Test MAPE: 112.0267\n",
      "Ridge Regression Run 3 Test RMSE: 0.2367\n",
      "Ridge Regression Run 3 Test sMAPE: 67.6426\n",
      "\n",
      "Ridge Regression Run 4 parameters: {'alpha': 1.5}\n",
      "Ridge Regression Run 4 Test MSE: 0.0560\n",
      "Ridge Regression Run 4 Test MAE: 0.2106\n",
      "Ridge Regression Run 4 Test R^2 Score: 0.0451\n",
      "Ridge Regression Run 4 Test MAPE: 112.0260\n",
      "Ridge Regression Run 4 Test RMSE: 0.2367\n",
      "Ridge Regression Run 4 Test sMAPE: 67.6536\n",
      "\n",
      "Ridge Regression Run 5 parameters: {'alpha': 2.0}\n",
      "Ridge Regression Run 5 Test MSE: 0.0560\n",
      "Ridge Regression Run 5 Test MAE: 0.2106\n",
      "Ridge Regression Run 5 Test R^2 Score: 0.0450\n",
      "Ridge Regression Run 5 Test MAPE: 112.0245\n",
      "Ridge Regression Run 5 Test RMSE: 0.2367\n",
      "Ridge Regression Run 5 Test sMAPE: 67.6637\n",
      "\n",
      "Ridge Regression Run 6 parameters: {'alpha': 10.0}\n",
      "Ridge Regression Run 6 Test MSE: 0.0562\n",
      "Ridge Regression Run 6 Test MAE: 0.2110\n",
      "Ridge Regression Run 6 Test R^2 Score: 0.0424\n",
      "Ridge Regression Run 6 Test MAPE: 112.0158\n",
      "Ridge Regression Run 6 Test RMSE: 0.2371\n",
      "Ridge Regression Run 6 Test sMAPE: 67.7924\n"
     ]
    }
   ],
   "source": [
    "ridge_reg = Ridge\n",
    "param_grid_ridge = {\n",
    "    'alpha': [0.1, 0.9, 1.0, 1.5, 2.0, 10.0]\n",
    "}\n",
    "\n",
    "train_and_evaluate_model(\n",
    "    model_class=ridge_reg,\n",
    "    param_grid=param_grid_ridge,\n",
    "    model_name=\"Ridge Regression\",\n",
    "    X_train=X_train_flat,\n",
    "    y_train=y_train_flat,\n",
    "    X_test=X_test_flat,\n",
    "    y_test=y_test_flat\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Lasso Regression Run 1 parameters: {'alpha': 0.0001, 'max_iter': 10000}\n",
      "Lasso Regression Run 1 Test MSE: 0.0563\n",
      "Lasso Regression Run 1 Test MAE: 0.2112\n",
      "Lasso Regression Run 1 Test R^2 Score: 0.0410\n",
      "Lasso Regression Run 1 Test MAPE: 111.9876\n",
      "Lasso Regression Run 1 Test RMSE: 0.2372\n",
      "Lasso Regression Run 1 Test sMAPE: 67.8755\n",
      "\n",
      "Lasso Regression Run 2 parameters: {'alpha': 0.001, 'max_iter': 10000}\n",
      "Lasso Regression Run 2 Test MSE: 0.0572\n",
      "Lasso Regression Run 2 Test MAE: 0.2150\n",
      "Lasso Regression Run 2 Test R^2 Score: 0.0260\n",
      "Lasso Regression Run 2 Test MAPE: 115.0582\n",
      "Lasso Regression Run 2 Test RMSE: 0.2391\n",
      "Lasso Regression Run 2 Test sMAPE: 68.8978\n",
      "\n",
      "Lasso Regression Run 3 parameters: {'alpha': 0.01, 'max_iter': 10000}\n",
      "Lasso Regression Run 3 Test MSE: 0.0583\n",
      "Lasso Regression Run 3 Test MAE: 0.2199\n",
      "Lasso Regression Run 3 Test R^2 Score: 0.0060\n",
      "Lasso Regression Run 3 Test MAPE: 118.7917\n",
      "Lasso Regression Run 3 Test RMSE: 0.2415\n",
      "Lasso Regression Run 3 Test sMAPE: 70.2226\n",
      "\n",
      "Lasso Regression Run 4 parameters: {'alpha': 0.1, 'max_iter': 10000}\n",
      "Lasso Regression Run 4 Test MSE: 0.0582\n",
      "Lasso Regression Run 4 Test MAE: 0.2196\n",
      "Lasso Regression Run 4 Test R^2 Score: 0.0079\n",
      "Lasso Regression Run 4 Test MAPE: 118.4334\n",
      "Lasso Regression Run 4 Test RMSE: 0.2413\n",
      "Lasso Regression Run 4 Test sMAPE: 70.1490\n",
      "\n",
      "Lasso Regression Run 5 parameters: {'alpha': 1.0, 'max_iter': 10000}\n",
      "Lasso Regression Run 5 Test MSE: 0.0581\n",
      "Lasso Regression Run 5 Test MAE: 0.2190\n",
      "Lasso Regression Run 5 Test R^2 Score: 0.0102\n",
      "Lasso Regression Run 5 Test MAPE: 117.7239\n",
      "Lasso Regression Run 5 Test RMSE: 0.2410\n",
      "Lasso Regression Run 5 Test sMAPE: 70.0111\n",
      "\n",
      "Lasso Regression Run 6 parameters: {'alpha': 1.5, 'max_iter': 10000}\n",
      "Lasso Regression Run 6 Test MSE: 0.0581\n",
      "Lasso Regression Run 6 Test MAE: 0.2191\n",
      "Lasso Regression Run 6 Test R^2 Score: 0.0096\n",
      "Lasso Regression Run 6 Test MAPE: 117.7553\n",
      "Lasso Regression Run 6 Test RMSE: 0.2411\n",
      "Lasso Regression Run 6 Test sMAPE: 70.0298\n",
      "\n",
      "Lasso Regression Run 7 parameters: {'alpha': 2.0, 'max_iter': 10000}\n",
      "Lasso Regression Run 7 Test MSE: 0.0582\n",
      "Lasso Regression Run 7 Test MAE: 0.2191\n",
      "Lasso Regression Run 7 Test R^2 Score: 0.0089\n",
      "Lasso Regression Run 7 Test MAPE: 117.7867\n",
      "Lasso Regression Run 7 Test RMSE: 0.2412\n",
      "Lasso Regression Run 7 Test sMAPE: 70.0484\n",
      "\n",
      "Lasso Regression Run 8 parameters: {'alpha': 10.0, 'max_iter': 10000}\n",
      "Lasso Regression Run 8 Test MSE: 0.0587\n",
      "Lasso Regression Run 8 Test MAE: 0.2199\n",
      "Lasso Regression Run 8 Test R^2 Score: -0.0000\n",
      "Lasso Regression Run 8 Test MAPE: 118.1538\n",
      "Lasso Regression Run 8 Test RMSE: 0.2422\n",
      "Lasso Regression Run 8 Test sMAPE: 70.2532\n"
     ]
    }
   ],
   "source": [
    "lasso_reg = Lasso\n",
    "param_grid_lasso = {\n",
    "    'alpha': [0.0001, 0.001, 0.01, 0.1, 1.0, 1.5, 2.0, 10.0],\n",
    "    'max_iter': [10000]\n",
    "}\n",
    "\n",
    "train_and_evaluate_model(\n",
    "    model_class=lasso_reg,\n",
    "    param_grid=param_grid_lasso,\n",
    "    model_name=\"Lasso Regression\",\n",
    "    X_train=X_train_flat,\n",
    "    y_train=y_train_flat,\n",
    "    X_test=X_test_flat,\n",
    "    y_test=y_test_flat\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Decision Tree Regressor Run 1 parameters: {'max_depth': None, 'min_samples_split': 2}\n",
      "Decision Tree Regressor Run 1 Test MSE: 0.1108\n",
      "Decision Tree Regressor Run 1 Test MAE: 0.2423\n",
      "Decision Tree Regressor Run 1 Test R^2 Score: -0.8878\n",
      "Decision Tree Regressor Run 1 Test MAPE: 127.2581\n",
      "Decision Tree Regressor Run 1 Test RMSE: 0.3328\n",
      "Decision Tree Regressor Run 1 Test sMAPE: 73.8828\n",
      "\n",
      "Decision Tree Regressor Run 2 parameters: {'max_depth': None, 'min_samples_split': 5}\n",
      "Decision Tree Regressor Run 2 Test MSE: 0.1087\n",
      "Decision Tree Regressor Run 2 Test MAE: 0.2436\n",
      "Decision Tree Regressor Run 2 Test R^2 Score: -0.8524\n",
      "Decision Tree Regressor Run 2 Test MAPE: 128.2310\n",
      "Decision Tree Regressor Run 2 Test RMSE: 0.3297\n",
      "Decision Tree Regressor Run 2 Test sMAPE: 73.6349\n",
      "\n",
      "Decision Tree Regressor Run 3 parameters: {'max_depth': None, 'min_samples_split': 10}\n",
      "Decision Tree Regressor Run 3 Test MSE: 0.0919\n",
      "Decision Tree Regressor Run 3 Test MAE: 0.2307\n",
      "Decision Tree Regressor Run 3 Test R^2 Score: -0.5656\n",
      "Decision Tree Regressor Run 3 Test MAPE: 118.8043\n",
      "Decision Tree Regressor Run 3 Test RMSE: 0.3031\n",
      "Decision Tree Regressor Run 3 Test sMAPE: 71.8612\n",
      "\n",
      "Decision Tree Regressor Run 4 parameters: {'max_depth': 5, 'min_samples_split': 2}\n",
      "Decision Tree Regressor Run 4 Test MSE: 0.0613\n",
      "Decision Tree Regressor Run 4 Test MAE: 0.2164\n",
      "Decision Tree Regressor Run 4 Test R^2 Score: -0.0448\n",
      "Decision Tree Regressor Run 4 Test MAPE: 113.3211\n",
      "Decision Tree Regressor Run 4 Test RMSE: 0.2476\n",
      "Decision Tree Regressor Run 4 Test sMAPE: 68.8924\n",
      "\n",
      "Decision Tree Regressor Run 5 parameters: {'max_depth': 5, 'min_samples_split': 5}\n",
      "Decision Tree Regressor Run 5 Test MSE: 0.0613\n",
      "Decision Tree Regressor Run 5 Test MAE: 0.2164\n",
      "Decision Tree Regressor Run 5 Test R^2 Score: -0.0448\n",
      "Decision Tree Regressor Run 5 Test MAPE: 113.3211\n",
      "Decision Tree Regressor Run 5 Test RMSE: 0.2476\n",
      "Decision Tree Regressor Run 5 Test sMAPE: 68.8924\n",
      "\n",
      "Decision Tree Regressor Run 6 parameters: {'max_depth': 5, 'min_samples_split': 10}\n",
      "Decision Tree Regressor Run 6 Test MSE: 0.0611\n",
      "Decision Tree Regressor Run 6 Test MAE: 0.2161\n",
      "Decision Tree Regressor Run 6 Test R^2 Score: -0.0409\n",
      "Decision Tree Regressor Run 6 Test MAPE: 113.2190\n",
      "Decision Tree Regressor Run 6 Test RMSE: 0.2471\n",
      "Decision Tree Regressor Run 6 Test sMAPE: 68.8651\n",
      "\n",
      "Decision Tree Regressor Run 7 parameters: {'max_depth': 10, 'min_samples_split': 2}\n",
      "Decision Tree Regressor Run 7 Test MSE: 0.0721\n",
      "Decision Tree Regressor Run 7 Test MAE: 0.2215\n",
      "Decision Tree Regressor Run 7 Test R^2 Score: -0.2285\n",
      "Decision Tree Regressor Run 7 Test MAPE: 115.6899\n",
      "Decision Tree Regressor Run 7 Test RMSE: 0.2685\n",
      "Decision Tree Regressor Run 7 Test sMAPE: 69.8111\n",
      "\n",
      "Decision Tree Regressor Run 8 parameters: {'max_depth': 10, 'min_samples_split': 5}\n",
      "Decision Tree Regressor Run 8 Test MSE: 0.0716\n",
      "Decision Tree Regressor Run 8 Test MAE: 0.2206\n",
      "Decision Tree Regressor Run 8 Test R^2 Score: -0.2196\n",
      "Decision Tree Regressor Run 8 Test MAPE: 115.1425\n",
      "Decision Tree Regressor Run 8 Test RMSE: 0.2675\n",
      "Decision Tree Regressor Run 8 Test sMAPE: 69.2303\n",
      "\n",
      "Decision Tree Regressor Run 9 parameters: {'max_depth': 10, 'min_samples_split': 10}\n",
      "Decision Tree Regressor Run 9 Test MSE: 0.0682\n",
      "Decision Tree Regressor Run 9 Test MAE: 0.2166\n",
      "Decision Tree Regressor Run 9 Test R^2 Score: -0.1623\n",
      "Decision Tree Regressor Run 9 Test MAPE: 113.2870\n",
      "Decision Tree Regressor Run 9 Test RMSE: 0.2612\n",
      "Decision Tree Regressor Run 9 Test sMAPE: 68.6276\n",
      "\n",
      "Decision Tree Regressor Run 10 parameters: {'max_depth': 20, 'min_samples_split': 2}\n",
      "Decision Tree Regressor Run 10 Test MSE: 0.1026\n",
      "Decision Tree Regressor Run 10 Test MAE: 0.2364\n",
      "Decision Tree Regressor Run 10 Test R^2 Score: -0.7479\n",
      "Decision Tree Regressor Run 10 Test MAPE: 123.3912\n",
      "Decision Tree Regressor Run 10 Test RMSE: 0.3203\n",
      "Decision Tree Regressor Run 10 Test sMAPE: 72.5231\n",
      "\n",
      "Decision Tree Regressor Run 11 parameters: {'max_depth': 20, 'min_samples_split': 5}\n",
      "Decision Tree Regressor Run 11 Test MSE: 0.1027\n",
      "Decision Tree Regressor Run 11 Test MAE: 0.2394\n",
      "Decision Tree Regressor Run 11 Test R^2 Score: -0.7501\n",
      "Decision Tree Regressor Run 11 Test MAPE: 122.7318\n",
      "Decision Tree Regressor Run 11 Test RMSE: 0.3205\n",
      "Decision Tree Regressor Run 11 Test sMAPE: 72.3642\n",
      "\n",
      "Decision Tree Regressor Run 12 parameters: {'max_depth': 20, 'min_samples_split': 10}\n",
      "Decision Tree Regressor Run 12 Test MSE: 0.0908\n",
      "Decision Tree Regressor Run 12 Test MAE: 0.2302\n",
      "Decision Tree Regressor Run 12 Test R^2 Score: -0.5474\n",
      "Decision Tree Regressor Run 12 Test MAPE: 118.5458\n",
      "Decision Tree Regressor Run 12 Test RMSE: 0.3013\n",
      "Decision Tree Regressor Run 12 Test sMAPE: 71.4473\n"
     ]
    }
   ],
   "source": [
    "decision_tree_reg = DecisionTreeRegressor\n",
    "param_grid_dtree_reg = {\n",
    "    'max_depth': [None, 5, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "train_and_evaluate_model(\n",
    "    model_class=decision_tree_reg,\n",
    "    param_grid=param_grid_dtree_reg,\n",
    "    model_name=\"Decision Tree Regressor\",\n",
    "    X_train=X_train_flat,\n",
    "    y_train=y_train_flat,\n",
    "    X_test=X_test_flat,\n",
    "    y_test=y_test_flat\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svr_model_class = SVR\n",
    "param_grid_svr = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'kernel': ['linear', 'rbf'],\n",
    "    'epsilon': [0.01, 0.1, 1]\n",
    "}\n",
    "\n",
    "train_and_evaluate_model(\n",
    "    model_class=svr_model_class,\n",
    "    param_grid=param_grid_svr,\n",
    "    model_name=\"Support Vector Regressor\",\n",
    "    X_train=X_train_flat,\n",
    "    y_train=y_train_flat,\n",
    "    X_test=X_test_flat,\n",
    "    y_test=y_test_flat\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_reg = RandomForestRegressor()\n",
    "param_grid_rf_reg = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [None, 5, 10],\n",
    "    'min_samples_split': [2, 5]\n",
    "}\n",
    "\n",
    "train_and_evaluate_model(\n",
    "    model=random_forest_reg,\n",
    "    param_grid=param_grid_rf_reg,\n",
    "    model_name=\"Random Forest Regressor\",\n",
    "    X_train=X_train_flat,\n",
    "    y_train=y_train_flat,\n",
    "    X_test=X_test_flat,\n",
    "    y_test=y_test_flat\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n"
     ]
    }
   ],
   "source": [
    "gb_regressor = GradientBoostingRegressor()\n",
    "param_grid_gb_reg = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'learning_rate': [0.01, 0.1],\n",
    "    'max_depth': [3, 5]\n",
    "}\n",
    "\n",
    "train_and_evaluate_model(\n",
    "    model=gb_regressor,\n",
    "    param_grid=param_grid_gb_reg,\n",
    "    model_name=\"Gradient Boosting Regressor\",\n",
    "    X_train=X_train_flat,\n",
    "    y_train=y_train_flat,\n",
    "    X_test=X_test_flat,\n",
    "    y_test=y_test_flat\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "XGBoost Regressor Run 1 parameters: {'n_estimators': 100, 'learning_rate': 0.01, 'max_depth': 3, 'objective': 'reg:squarederror'}\n",
      "XGBoost Regressor Run 1 Test MSE: 0.0572\n",
      "XGBoost Regressor Run 1 Test MAE: 0.2148\n",
      "XGBoost Regressor Run 1 Test R^2 Score: 0.0259\n",
      "XGBoost Regressor Run 1 Test MAPE: 113.9762\n",
      "XGBoost Regressor Run 1 Test RMSE: 0.2391\n",
      "XGBoost Regressor Run 1 Test sMAPE: 68.9682\n",
      "\n",
      "XGBoost Regressor Run 2 parameters: {'n_estimators': 100, 'learning_rate': 0.01, 'max_depth': 5, 'objective': 'reg:squarederror'}\n",
      "XGBoost Regressor Run 2 Test MSE: 0.0571\n",
      "XGBoost Regressor Run 2 Test MAE: 0.2140\n",
      "XGBoost Regressor Run 2 Test R^2 Score: 0.0275\n",
      "XGBoost Regressor Run 2 Test MAPE: 113.7460\n",
      "XGBoost Regressor Run 2 Test RMSE: 0.2389\n",
      "XGBoost Regressor Run 2 Test sMAPE: 68.6829\n",
      "\n",
      "XGBoost Regressor Run 3 parameters: {'n_estimators': 100, 'learning_rate': 0.1, 'max_depth': 3, 'objective': 'reg:squarederror'}\n",
      "XGBoost Regressor Run 3 Test MSE: 0.0560\n",
      "XGBoost Regressor Run 3 Test MAE: 0.2047\n",
      "XGBoost Regressor Run 3 Test R^2 Score: 0.0455\n",
      "XGBoost Regressor Run 3 Test MAPE: 107.1268\n",
      "XGBoost Regressor Run 3 Test RMSE: 0.2367\n",
      "XGBoost Regressor Run 3 Test sMAPE: 66.0860\n",
      "\n",
      "XGBoost Regressor Run 4 parameters: {'n_estimators': 100, 'learning_rate': 0.1, 'max_depth': 5, 'objective': 'reg:squarederror'}\n",
      "XGBoost Regressor Run 4 Test MSE: 0.0568\n",
      "XGBoost Regressor Run 4 Test MAE: 0.2033\n",
      "XGBoost Regressor Run 4 Test R^2 Score: 0.0328\n",
      "XGBoost Regressor Run 4 Test MAPE: 106.1542\n",
      "XGBoost Regressor Run 4 Test RMSE: 0.2382\n",
      "XGBoost Regressor Run 4 Test sMAPE: 65.6255\n",
      "\n",
      "XGBoost Regressor Run 5 parameters: {'n_estimators': 200, 'learning_rate': 0.01, 'max_depth': 3, 'objective': 'reg:squarederror'}\n",
      "XGBoost Regressor Run 5 Test MSE: 0.0561\n",
      "XGBoost Regressor Run 5 Test MAE: 0.2114\n",
      "XGBoost Regressor Run 5 Test R^2 Score: 0.0436\n",
      "XGBoost Regressor Run 5 Test MAPE: 111.9025\n",
      "XGBoost Regressor Run 5 Test RMSE: 0.2369\n",
      "XGBoost Regressor Run 5 Test sMAPE: 68.0615\n",
      "\n",
      "XGBoost Regressor Run 6 parameters: {'n_estimators': 200, 'learning_rate': 0.01, 'max_depth': 5, 'objective': 'reg:squarederror'}\n",
      "XGBoost Regressor Run 6 Test MSE: 0.0557\n",
      "XGBoost Regressor Run 6 Test MAE: 0.2087\n",
      "XGBoost Regressor Run 6 Test R^2 Score: 0.0504\n",
      "XGBoost Regressor Run 6 Test MAPE: 111.1046\n",
      "XGBoost Regressor Run 6 Test RMSE: 0.2361\n",
      "XGBoost Regressor Run 6 Test sMAPE: 67.2705\n",
      "\n",
      "XGBoost Regressor Run 7 parameters: {'n_estimators': 200, 'learning_rate': 0.1, 'max_depth': 3, 'objective': 'reg:squarederror'}\n",
      "XGBoost Regressor Run 7 Test MSE: 0.0568\n",
      "XGBoost Regressor Run 7 Test MAE: 0.2032\n",
      "XGBoost Regressor Run 7 Test R^2 Score: 0.0316\n",
      "XGBoost Regressor Run 7 Test MAPE: 105.4697\n",
      "XGBoost Regressor Run 7 Test RMSE: 0.2384\n",
      "XGBoost Regressor Run 7 Test sMAPE: 65.5679\n",
      "\n",
      "XGBoost Regressor Run 8 parameters: {'n_estimators': 200, 'learning_rate': 0.1, 'max_depth': 5, 'objective': 'reg:squarederror'}\n",
      "XGBoost Regressor Run 8 Test MSE: 0.0579\n",
      "XGBoost Regressor Run 8 Test MAE: 0.2036\n",
      "XGBoost Regressor Run 8 Test R^2 Score: 0.0136\n",
      "XGBoost Regressor Run 8 Test MAPE: 106.7382\n",
      "XGBoost Regressor Run 8 Test RMSE: 0.2406\n",
      "XGBoost Regressor Run 8 Test sMAPE: 65.5570\n"
     ]
    }
   ],
   "source": [
    "xgboost_reg = xgb.XGBRegressor\n",
    "param_grid_xgb_reg = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'learning_rate': [0.01, 0.1],\n",
    "    'max_depth': [3, 5],\n",
    "    'objective': ['reg:squarederror']\n",
    "}\n",
    "\n",
    "train_and_evaluate_model(\n",
    "    model_class=xgboost_reg,\n",
    "    param_grid=param_grid_xgb_reg,\n",
    "    model_name=\"XGBoost Regressor\",\n",
    "    X_train=X_train_flat,\n",
    "    y_train=y_train_flat,\n",
    "    X_test=X_test_flat,\n",
    "    y_test=y_test_flat\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.251038 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3654\n",
      "[LightGBM] [Info] Number of data points in the train set: 219147, number of used features: 145\n",
      "[LightGBM] [Info] Start training from score 0.006051\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.086654 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3654\n",
      "[LightGBM] [Info] Number of data points in the train set: 219147, number of used features: 145\n",
      "[LightGBM] [Info] Start training from score 0.006051\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.202939 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3654\n",
      "[LightGBM] [Info] Number of data points in the train set: 219147, number of used features: 145\n",
      "[LightGBM] [Info] Start training from score 0.006051\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.041435 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3701\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.042858 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3701\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.031709 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3714\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019379 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Number of data points in the train set: 219146, number of used features: 166\n",
      "[LightGBM] [Info] Total Bins 3701\n",
      "[LightGBM] [Info] Number of data points in the train set: 219146, number of used features: 166\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.021993 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3701\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.070045 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3714\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.048020 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Number of data points in the train set: 219147, number of used features: 167\n",
      "[LightGBM] [Info] Total Bins 3714\n",
      "[LightGBM] [Info] Number of data points in the train set: 219146, number of used features: 166\n",
      "[LightGBM] [Info] Number of data points in the train set: 219147, number of used features: 167\n",
      "[LightGBM] [Info] Number of data points in the train set: 219146, number of used features: 166\n",
      "[LightGBM] [Info] Number of data points in the train set: 219147, number of used features: 167\n",
      "[LightGBM] [Info] Start training from score 0.006283\n",
      "[LightGBM] [Info] Start training from score 0.006283\n",
      "[LightGBM] [Info] Start training from score 0.006229\n",
      "[LightGBM] [Info] Start training from score 0.006283\n",
      "[LightGBM] [Info] Start training from score 0.006283\n",
      "[LightGBM] [Info] Start training from score 0.006229\n",
      "[LightGBM] [Info] Start training from score 0.006229\n",
      "[CV 2/3] END learning_rate=0.01, n_estimators=100, num_leaves=31;, score=-0.002 total time=  15.5s\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012872 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3714\n",
      "[LightGBM] [Info] Number of data points in the train set: 219147, number of used features: 167\n",
      "[LightGBM] [Info] Start training from score 0.006229\n",
      "[CV 1/3] END learning_rate=0.01, n_estimators=100, num_leaves=31;, score=-0.002 total time=  17.3s\n",
      "[CV 3/3] END learning_rate=0.01, n_estimators=100, num_leaves=31;, score=-0.003 total time=  17.3s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005872 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3654\n",
      "[LightGBM] [Info] Number of data points in the train set: 219147, number of used features: 145\n",
      "[LightGBM] [Info] Start training from score 0.006051\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005673 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3701\n",
      "[LightGBM] [Info] Number of data points in the train set: 219146, number of used features: 166\n",
      "[LightGBM] [Info] Start training from score 0.006283\n",
      "[CV 3/3] END learning_rate=0.01, n_estimators=100, num_leaves=63;, score=-0.003 total time=  24.9s\n",
      "[CV 2/3] END learning_rate=0.01, n_estimators=200, num_leaves=31;, score=-0.002 total time=  25.3s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005655 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3714\n",
      "[LightGBM] [Info] Number of data points in the train set: 219147, number of used features: 167\n",
      "[LightGBM] [Info] Start training from score 0.006229\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005515 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3654\n",
      "[LightGBM] [Info] Number of data points in the train set: 219147, number of used features: 145\n",
      "[LightGBM] [Info] Start training from score 0.006051\n",
      "[CV 1/3] END learning_rate=0.01, n_estimators=100, num_leaves=63;, score=-0.002 total time=  28.3s\n",
      "[CV 3/3] END learning_rate=0.01, n_estimators=200, num_leaves=31;, score=-0.003 total time=  28.2s\n",
      "[CV 1/3] END learning_rate=0.01, n_estimators=200, num_leaves=31;, score=-0.002 total time=  28.9s\n",
      "[CV 2/3] END learning_rate=0.01, n_estimators=100, num_leaves=63;, score=-0.002 total time=  29.2s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006131 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3701\n",
      "[LightGBM] [Info] Number of data points in the train set: 219146, number of used features: 166\n",
      "[LightGBM] [Info] Start training from score 0.006283\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.019823 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3714\n",
      "[LightGBM] [Info] Number of data points in the train set: 219147, number of used features: 167\n",
      "[LightGBM] [Info] Start training from score 0.006229\n",
      "[CV 1/3] END learning_rate=0.1, n_estimators=100, num_leaves=31;, score=-0.002 total time=  12.7s\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010809 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3654\n",
      "[LightGBM] [Info] Number of data points in the train set: 219147, number of used features: 145\n",
      "[LightGBM] [Info] Start training from score 0.006051\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.021886 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3701\n",
      "[LightGBM] [Info] Number of data points in the train set: 219146, number of used features: 166\n",
      "[LightGBM] [Info] Start training from score 0.006283\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006039 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3714\n",
      "[LightGBM] [Info] Number of data points in the train set: 219147, number of used features: 167\n",
      "[LightGBM] [Info] Start training from score 0.006229\n",
      "[CV 2/3] END learning_rate=0.1, n_estimators=100, num_leaves=31;, score=-0.002 total time=  12.8s\n",
      "[CV 3/3] END learning_rate=0.1, n_estimators=100, num_leaves=31;, score=-0.003 total time=  12.8s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005263 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3654\n",
      "[LightGBM] [Info] Number of data points in the train set: 219147, number of used features: 145\n",
      "[LightGBM] [Info] Start training from score 0.006051\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006385 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3701\n",
      "[LightGBM] [Info] Number of data points in the train set: 219146, number of used features: 166\n",
      "[LightGBM] [Info] Start training from score 0.006283\n",
      "[CV 2/3] END learning_rate=0.1, n_estimators=100, num_leaves=63;, score=-0.002 total time=  19.8s\n",
      "[CV 3/3] END learning_rate=0.1, n_estimators=100, num_leaves=63;, score=-0.003 total time=  19.7s\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011359 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3714\n",
      "[LightGBM] [Info] Number of data points in the train set: 219147, number of used features: 167\n",
      "[LightGBM] [Info] Start training from score 0.006229\n",
      "[CV 1/3] END learning_rate=0.1, n_estimators=200, num_leaves=31;, score=-0.002 total time=  20.1s\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012159 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3654\n",
      "[LightGBM] [Info] Number of data points in the train set: 219147, number of used features: 145\n",
      "[LightGBM] [Info] Start training from score 0.006051\n",
      "[CV 1/3] END learning_rate=0.01, n_estimators=200, num_leaves=63;, score=-0.002 total time=  51.3s\n",
      "[CV 1/3] END learning_rate=0.1, n_estimators=100, num_leaves=63;, score=-0.002 total time=  23.5s\n",
      "[CV 2/3] END learning_rate=0.1, n_estimators=200, num_leaves=31;, score=-0.002 total time=  22.7s\n",
      "[CV 2/3] END learning_rate=0.01, n_estimators=200, num_leaves=63;, score=-0.002 total time=  39.0s\n",
      "[CV 3/3] END learning_rate=0.1, n_estimators=200, num_leaves=31;, score=-0.003 total time=  19.2s\n",
      "[CV 3/3] END learning_rate=0.01, n_estimators=200, num_leaves=63;, score=-0.003 total time=  40.7s\n",
      "[CV 1/3] END learning_rate=0.1, n_estimators=200, num_leaves=63;, score=-0.002 total time=  24.3s\n",
      "[CV 2/3] END learning_rate=0.1, n_estimators=200, num_leaves=63;, score=-0.002 total time=  14.8s\n",
      "[CV 3/3] END learning_rate=0.1, n_estimators=200, num_leaves=63;, score=-0.003 total time=  14.6s\n"
     ]
    }
   ],
   "source": [
    "lgbm_reg = lgb.LGBMRegressor()\n",
    "param_grid_lgbm_reg = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'learning_rate': [0.01, 0.1],\n",
    "    'num_leaves': [31, 63]\n",
    "}\n",
    "\n",
    "train_and_evaluate_model(\n",
    "    model=lgbm_reg,\n",
    "    param_grid=param_grid_lgbm_reg,\n",
    "    model_name=\"LightGBM Regressor\",\n",
    "    X_train=X_train_flat,\n",
    "    y_train=y_train_flat,\n",
    "    X_test=X_test_flat,\n",
    "    y_test=y_test_flat\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 9\u001b[0m\n\u001b[1;32m      3\u001b[0m knn_model \u001b[38;5;241m=\u001b[39m KNeighborsRegressor()\n\u001b[1;32m      4\u001b[0m param_grid_knn \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_neighbors\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m7\u001b[39m],\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweights\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muniform\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdistance\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      7\u001b[0m }\n\u001b[0;32m----> 9\u001b[0m \u001b[43mtrain_and_evaluate_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mknn_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparam_grid\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparam_grid_knn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mK-Nearest Neighbors Regressor\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_train_flat\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_train_flat\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_test\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_test_flat\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_test\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_test_flat\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 15\u001b[0m, in \u001b[0;36mtrain_and_evaluate_model\u001b[0;34m(model, param_grid, model_name, X_train, y_train, X_test, y_test)\u001b[0m\n\u001b[1;32m      5\u001b[0m grid_search \u001b[38;5;241m=\u001b[39m GridSearchCV(\n\u001b[1;32m      6\u001b[0m     estimator\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m      7\u001b[0m     param_grid\u001b[38;5;241m=\u001b[39mparam_grid,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m\n\u001b[1;32m     12\u001b[0m )\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Fit the model\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m \u001b[43mgrid_search\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Best parameters\u001b[39;00m\n\u001b[1;32m     18\u001b[0m best_params \u001b[38;5;241m=\u001b[39m grid_search\u001b[38;5;241m.\u001b[39mbest_params_\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/sklearn/base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1150\u001b[0m     )\n\u001b[1;32m   1151\u001b[0m ):\n\u001b[0;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/sklearn/model_selection/_search.py:898\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    892\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[1;32m    893\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m    894\u001b[0m     )\n\u001b[1;32m    896\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[0;32m--> 898\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    900\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m    901\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m    902\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/sklearn/model_selection/_search.py:1422\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1420\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1421\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1422\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/sklearn/model_selection/_search.py:845\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    837\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    838\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    839\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    840\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    841\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[1;32m    842\u001b[0m         )\n\u001b[1;32m    843\u001b[0m     )\n\u001b[0;32m--> 845\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    846\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    847\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    848\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    849\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    850\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    851\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    852\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    853\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    854\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    855\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    856\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    858\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    859\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    863\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    864\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    865\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    866\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    867\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/sklearn/utils/parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     60\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     61\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     64\u001b[0m )\n\u001b[0;32m---> 65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/joblib/parallel.py:1952\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1946\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[1;32m   1947\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[1;32m   1948\u001b[0m \u001b[38;5;66;03m# reach the first `yield` statement. This starts the aynchronous\u001b[39;00m\n\u001b[1;32m   1949\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[1;32m   1950\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 1952\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/joblib/parallel.py:1595\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1592\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m   1594\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1595\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[1;32m   1597\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[1;32m   1598\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[1;32m   1599\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[1;32m   1600\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[1;32m   1601\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/joblib/parallel.py:1707\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1702\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[1;32m   1703\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[1;32m   1704\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m   1705\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[1;32m   1706\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[0;32m-> 1707\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.01\u001b[39m)\n\u001b[1;32m   1708\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   1710\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[1;32m   1711\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[1;32m   1712\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "knn_model = KNeighborsRegressor()\n",
    "param_grid_knn = {\n",
    "    'n_neighbors': [3, 5, 7],\n",
    "    'weights': ['uniform', 'distance']\n",
    "}\n",
    "\n",
    "train_and_evaluate_model(\n",
    "    model=knn_model,\n",
    "    param_grid=param_grid_knn,\n",
    "    model_name=\"K-Nearest Neighbors Regressor\",\n",
    "    X_train=X_train_flat,\n",
    "    y_train=y_train_flat,\n",
    "    X_test=X_test_flat,\n",
    "    y_test=y_test_flat\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RaceRegressionModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=128):\n",
    "        super(RaceRegressionModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, 1)  # Output a score for each rider\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x should have shape (batch_size, num_features)\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class RaceRegressionDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)  # Shape: (num_samples, num_features)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)  # Shape: (num_samples,)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        X = self.X[idx]\n",
    "        y = self.y[idx]\n",
    "        return X, y\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = RaceRegressionDataset(X_train_flat, y_train_flat)\n",
    "test_dataset = RaceRegressionDataset(X_test_flat, y_test_flat)\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 256\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-30 14:18:56,872] A new study created in memory with name: no-name-95eb98f6-ec3b-41e5-a798-fe8c3d25536d\n",
      "[I 2024-11-30 14:19:01,104] Trial 0 finished with value: -0.030196388542649277 and parameters: {'hidden_size': 256, 'learning_rate': 0.009337652427237754, 'weight_decay': 0.00019415087673350546, 'num_epochs': 17, 'batch_size': 128}. Best is trial 0 with value: -0.030196388542649277.\n",
      "[I 2024-11-30 14:19:05,453] Trial 1 finished with value: -0.04745134185417199 and parameters: {'hidden_size': 256, 'learning_rate': 0.009689776321021842, 'weight_decay': 0.0004671726918041624, 'num_epochs': 18, 'batch_size': 64}. Best is trial 0 with value: -0.030196388542649277.\n",
      "[I 2024-11-30 14:19:08,824] Trial 2 finished with value: 0.018673010527187617 and parameters: {'hidden_size': 256, 'learning_rate': 0.0032667564866602447, 'weight_decay': 0.00042415680311678106, 'num_epochs': 12, 'batch_size': 128}. Best is trial 2 with value: 0.018673010527187617.\n",
      "[I 2024-11-30 14:19:13,708] Trial 3 finished with value: -1.7335291947073461 and parameters: {'hidden_size': 128, 'learning_rate': 0.0045656349660448126, 'weight_decay': 0.0008351936802016589, 'num_epochs': 29, 'batch_size': 64}. Best is trial 2 with value: 0.018673010527187617.\n",
      "[I 2024-11-30 14:19:17,837] Trial 4 finished with value: -6.594537243444376 and parameters: {'hidden_size': 128, 'learning_rate': 0.0039721506300617535, 'weight_decay': 0.00044025508272087466, 'num_epochs': 27, 'batch_size': 128}. Best is trial 2 with value: 0.018673010527187617.\n",
      "[I 2024-11-30 14:19:21,817] Trial 5 finished with value: -0.006411515500475806 and parameters: {'hidden_size': 128, 'learning_rate': 0.005883671581839482, 'weight_decay': 0.0003974421584442536, 'num_epochs': 19, 'batch_size': 64}. Best is trial 2 with value: 0.018673010527187617.\n",
      "[I 2024-11-30 14:19:26,526] Trial 6 finished with value: -0.4026583940468871 and parameters: {'hidden_size': 256, 'learning_rate': 0.0006577194472187398, 'weight_decay': 0.0006380833554690196, 'num_epochs': 21, 'batch_size': 64}. Best is trial 2 with value: 0.018673010527187617.\n",
      "[I 2024-11-30 14:19:29,934] Trial 7 finished with value: -0.0812686324630727 and parameters: {'hidden_size': 256, 'learning_rate': 0.007557645052083412, 'weight_decay': 0.0006232448417133643, 'num_epochs': 11, 'batch_size': 128}. Best is trial 2 with value: 0.018673010527187617.\n",
      "[I 2024-11-30 14:19:34,257] Trial 8 finished with value: -4.002808494916794 and parameters: {'hidden_size': 128, 'learning_rate': 0.002910594806717465, 'weight_decay': 0.0009444416792920939, 'num_epochs': 22, 'batch_size': 64}. Best is trial 2 with value: 0.018673010527187617.\n",
      "[I 2024-11-30 14:19:37,543] Trial 9 finished with value: 0.022289420531112047 and parameters: {'hidden_size': 64, 'learning_rate': 0.001835439289842612, 'weight_decay': 0.0007743985352066165, 'num_epochs': 19, 'batch_size': 256}. Best is trial 9 with value: 0.022289420531112047.\n",
      "[I 2024-11-30 14:19:41,114] Trial 10 finished with value: -0.007361348528594824 and parameters: {'hidden_size': 64, 'learning_rate': 0.000342915171514975, 'weight_decay': 0.0007703781766348031, 'num_epochs': 24, 'batch_size': 256}. Best is trial 9 with value: 0.022289420531112047.\n",
      "[I 2024-11-30 14:19:44,277] Trial 11 finished with value: 0.008763763273975989 and parameters: {'hidden_size': 64, 'learning_rate': 0.002577245343090292, 'weight_decay': 0.00021593858248169227, 'num_epochs': 12, 'batch_size': 256}. Best is trial 9 with value: 0.022289420531112047.\n",
      "[I 2024-11-30 14:19:47,409] Trial 12 finished with value: -0.008919614202772541 and parameters: {'hidden_size': 64, 'learning_rate': 0.0020031877509745126, 'weight_decay': 7.050239559302861e-05, 'num_epochs': 14, 'batch_size': 256}. Best is trial 9 with value: 0.022289420531112047.\n",
      "[I 2024-11-30 14:19:50,523] Trial 13 finished with value: -0.021473223936150365 and parameters: {'hidden_size': 64, 'learning_rate': 0.005867764332451505, 'weight_decay': 0.0006603771716012138, 'num_epochs': 15, 'batch_size': 256}. Best is trial 9 with value: 0.022289420531112047.\n",
      "[I 2024-11-30 14:19:53,818] Trial 14 finished with value: -0.02908796225608601 and parameters: {'hidden_size': 256, 'learning_rate': 0.001625727011223312, 'weight_decay': 0.0009949074846869223, 'num_epochs': 10, 'batch_size': 128}. Best is trial 9 with value: 0.022289420531112047.\n",
      "[I 2024-11-30 14:19:56,933] Trial 15 finished with value: 0.020508236200362573 and parameters: {'hidden_size': 64, 'learning_rate': 0.0035118497279110457, 'weight_decay': 0.00032515794038101553, 'num_epochs': 15, 'batch_size': 256}. Best is trial 9 with value: 0.022289420531112047.\n",
      "[I 2024-11-30 14:20:00,642] Trial 16 finished with value: 0.025214471307838937 and parameters: {'hidden_size': 64, 'learning_rate': 0.005325027190633813, 'weight_decay': 0.0002716887611602172, 'num_epochs': 24, 'batch_size': 256}. Best is trial 16 with value: 0.025214471307838937.\n",
      "[I 2024-11-30 14:20:04,299] Trial 17 finished with value: 0.045610581602939515 and parameters: {'hidden_size': 64, 'learning_rate': 0.007273650046851983, 'weight_decay': 2.71692033484962e-05, 'num_epochs': 26, 'batch_size': 256}. Best is trial 17 with value: 0.045610581602939515.\n",
      "[I 2024-11-30 14:20:08,152] Trial 18 finished with value: 0.05288650204434975 and parameters: {'hidden_size': 64, 'learning_rate': 0.007854076339347725, 'weight_decay': 6.8048578595835654e-06, 'num_epochs': 26, 'batch_size': 256}. Best is trial 18 with value: 0.05288650204434975.\n",
      "[I 2024-11-30 14:20:12,042] Trial 19 finished with value: -0.8965733322612344 and parameters: {'hidden_size': 64, 'learning_rate': 0.008189476938590979, 'weight_decay': 2.6034525438525638e-05, 'num_epochs': 30, 'batch_size': 256}. Best is trial 18 with value: 0.05288650204434975.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'hidden_size': 64, 'learning_rate': 0.007854076339347725, 'weight_decay': 6.8048578595835654e-06, 'num_epochs': 26, 'batch_size': 256}\n",
      "Best sMAPE: 0.05288650204434975\n"
     ]
    }
   ],
   "source": [
    "# Define the objective function for Optuna\n",
    "def objective(trial):\n",
    "    # Hyperparameter suggestions\n",
    "    hidden_size = trial.suggest_categorical('hidden_size', [64, 128, 256])\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-2)\n",
    "    weight_decay = trial.suggest_float('weight_decay', 1e-6, 1e-3)\n",
    "    num_epochs = trial.suggest_int('num_epochs', 10, 30)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [64, 128, 256])\n",
    "\n",
    "    # Create data loaders with the suggested batch size\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    input_size = X_train_flat.shape[1]\n",
    "\n",
    "    # Initialize model, loss function, and optimizer\n",
    "    model = RaceRegressionModel(input_size, hidden_size).to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "    # Start MLflow run\n",
    "    with mlflow.start_run(run_name=f\"Neural Network R2 Trial {trial.number}\"):\n",
    "        mlflow.log_params({\n",
    "            'model_class': 'RaceRegressionModel',\n",
    "            'hidden_size': hidden_size,\n",
    "            'learning_rate': learning_rate,\n",
    "            'weight_decay': weight_decay,\n",
    "            'num_epochs': num_epochs,\n",
    "            'batch_size': batch_size\n",
    "        })\n",
    "\n",
    "        # Training loop\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            total_loss = 0\n",
    "            for X_batch, y_batch in train_loader:\n",
    "                X_batch = X_batch.to(device)\n",
    "                y_batch = y_batch.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(X_batch)\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                total_loss += loss.item() * X_batch.size(0)\n",
    "\n",
    "            average_loss = total_loss / len(train_loader.dataset)\n",
    "            mlflow.log_metric(\"train_loss\", average_loss, step=epoch)\n",
    "\n",
    "        # Evaluation on test set\n",
    "        model.eval()\n",
    "        y_true_list = []\n",
    "        y_pred_list = []\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in test_loader:\n",
    "                X_batch = X_batch.to(device)\n",
    "                y_batch = y_batch.to(device)\n",
    "\n",
    "                outputs = model(X_batch)\n",
    "                y_true_list.extend(y_batch.cpu().numpy())\n",
    "                y_pred_list.extend(outputs.cpu().numpy())\n",
    "\n",
    "        y_true_array = np.array(y_true_list)\n",
    "        y_pred_array = np.array(y_pred_list)\n",
    "\n",
    "        # Compute evaluation metrics\n",
    "        mse = mean_squared_error(y_true_array, y_pred_array)\n",
    "        mae = mean_absolute_error(y_true_array, y_pred_array)\n",
    "        rmse = np.sqrt(mse)\n",
    "        r2 = r2_score(y_true_array, y_pred_array)\n",
    "        mape = mean_absolute_percentage_error(y_true_array, y_pred_array)\n",
    "        smape = symmetric_mean_absolute_percentage_error(y_true_array, y_pred_array)\n",
    "\n",
    "        # Log metrics\n",
    "        mlflow.log_metrics({\n",
    "            'test_mse': mse,\n",
    "            'test_mae': mae,\n",
    "            'test_rmse': rmse,\n",
    "            'test_r2': r2,\n",
    "            'test_mape': mape,\n",
    "            'test_smape': smape\n",
    "        })\n",
    "\n",
    "        # Log the model\n",
    "        input_example = X_train_flat[:5].astype(np.float32)\n",
    "        input_example_tensor = torch.tensor(input_example, dtype=torch.float32).to(device)\n",
    "        signature = infer_signature(\n",
    "            input_example,\n",
    "            model(input_example_tensor).cpu().detach().numpy()\n",
    "        )\n",
    "        mlflow.pytorch.log_model(\n",
    "            pytorch_model=model,\n",
    "            artifact_path=\"model\",\n",
    "            input_example=input_example,\n",
    "            signature=signature\n",
    "        )\n",
    "\n",
    "    # Return the metric to optimize\n",
    "    return r2\n",
    "\n",
    "# Create an Optuna study and optimize\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=20)\n",
    "\n",
    "# Print best hyperparameters\n",
    "print(\"Best hyperparameters:\", study.best_params)\n",
    "print(\"Best sMAPE:\", study.best_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Improving your model's prediction performance involves a multifaceted approach that encompasses data quality, feature engineering, model architecture, training strategies, and evaluation techniques. Below are comprehensive strategies tailored to enhance your current setup:\n",
    "\n",
    "---\n",
    "\n",
    "## **1. Enhance Data Quality and Quantity**\n",
    "\n",
    "### **a. Increase Data Volume**\n",
    "- **More Data:** More training data can help your model generalize better. If possible, collect additional race and rider data.\n",
    "- **Data Augmentation:** For structured data, consider techniques like SMOTE (Synthetic Minority Over-sampling Technique) to generate synthetic samples, especially if dealing with class imbalance.\n",
    "\n",
    "### **b. Data Cleaning**\n",
    "- **Handle Missing Values:** Ensure all missing values are appropriately imputed or removed. You've used `SimpleImputer`, which is good, but verify the imputation strategy for each feature.\n",
    "- **Remove Outliers:** Identify and handle outliers that might skew the training process.\n",
    "- **Consistency Checks:** Ensure data consistency across different features and sources.\n",
    "\n",
    "### **c. Feature Scaling**\n",
    "- **Verify Scaling:** Ensure that all numerical features are properly scaled. You’re using `MinMaxScaler`, but sometimes `StandardScaler` can be more effective depending on the feature distribution.\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Advanced Feature Engineering**\n",
    "\n",
    "### **a. Feature Selection**\n",
    "- **Correlation Analysis:** Perform correlation analysis to identify and remove redundant features.\n",
    "- **Feature Importance:** Use techniques like feature importance from tree-based models to select the most impactful features.\n",
    "\n",
    "### **b. Create Interaction Features**\n",
    "- **Polynomial Features:** Generate polynomial combinations of existing features to capture non-linear relationships.\n",
    "- **Interaction Terms:** Create interaction terms between race-level and rider-level features (e.g., how rider **age** interacts with race **distance**).\n",
    "\n",
    "### **c. Categorical Encoding Enhancements**\n",
    "- **Embeddings:** Instead of one-hot encoding, use embeddings for high-cardinality categorical features like `rider_name`, `team`, and `nationality`. This approach reduces dimensionality and captures semantic relationships.\n",
    "  \n",
    "  ```python\n",
    "  from sklearn.preprocessing import LabelEncoder\n",
    "  \n",
    "  # Example for rider_name\n",
    "  label_encoder = LabelEncoder()\n",
    "  train_data['rider_name_encoded'] = label_encoder.fit_transform(train_data['rider_name'])\n",
    "  test_data['rider_name_encoded'] = label_encoder.transform(test_data['rider_name'])\n",
    "  ```\n",
    "\n",
    "### **d. Temporal Features**\n",
    "- **Year-Based Features:** Extract features like the number of years a rider has been active or changes in team affiliation over the years.\n",
    "- **Recent Performance:** Incorporate recent race performances to capture momentum.\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Optimize Model Architecture**\n",
    "\n",
    "### **a. Experiment with Different Models**\n",
    "\n",
    "- **Gradient Boosting Machines (GBMs):**\n",
    "  Models like **XGBoost**, **LightGBM**, or **CatBoost** often outperform neural networks on structured data.\n",
    "  \n",
    "  ```python\n",
    "  import lightgbm as lgb\n",
    "  from sklearn.model_selection import train_test_split\n",
    "  from sklearn.metrics import log_loss\n",
    "  \n",
    "  X_train_flat = X_train.reshape(X_train.shape[0], -1)\n",
    "  X_test_flat = X_test.reshape(X_test.shape[0], -1)\n",
    "  \n",
    "  lgb_train = lgb.Dataset(X_train_flat, y_train)\n",
    "  lgb_eval = lgb.Dataset(X_test_flat, y_test, reference=lgb_train)\n",
    "  \n",
    "  params = {\n",
    "      'objective': 'multiclass',  # or 'binary' depending on your task\n",
    "      'num_class': y_train.shape[1],  # for multiclass\n",
    "      'metric': 'multi_logloss',\n",
    "      'boosting_type': 'gbdt',\n",
    "      'learning_rate': 0.05,\n",
    "      'num_leaves': 31,\n",
    "      'verbose': -1\n",
    "  }\n",
    "  \n",
    "  gbm = lgb.train(params,\n",
    "                  lgb_train,\n",
    "                  num_boost_round=1000,\n",
    "                  valid_sets=[lgb_train, lgb_eval],\n",
    "                  early_stopping_rounds=50)\n",
    "  \n",
    "  y_pred = gbm.predict(X_test_flat, num_iteration=gbm.best_iteration)\n",
    "  print(f'Log Loss: {log_loss(y_test, y_pred)}')\n",
    "  ```\n",
    "\n",
    "- **Deep Learning Models:**\n",
    "  - **Deeper Networks:** Add more hidden layers or units to your current neural network.\n",
    "  - **Regularization Techniques:** Incorporate dropout layers or batch normalization to prevent overfitting.\n",
    "  - **Advanced Architectures:** Utilize architectures like **Multi-Layer Perceptrons (MLPs)** with residual connections or **Attention Mechanisms** to better capture complex relationships.\n",
    "\n",
    "### **b. Model Ensemble**\n",
    "- **Combine Multiple Models:** Create an ensemble of different models (e.g., neural networks, GBMs, logistic regression) to leverage their strengths.\n",
    "- **Stacking:** Use predictions from multiple models as input features for a meta-model.\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Refine Training Strategies**\n",
    "\n",
    "### **a. Hyperparameter Tuning**\n",
    "- **Automated Search:** Use tools like **Grid Search**, **Random Search**, or **Bayesian Optimization** (e.g., **Optuna**, **Hyperopt**) to find optimal hyperparameters.\n",
    "  \n",
    "  ```python\n",
    "  from sklearn.model_selection import GridSearchCV\n",
    "  from sklearn.ensemble import GradientBoostingClassifier\n",
    "  \n",
    "  param_grid = {\n",
    "      'n_estimators': [100, 200],\n",
    "      'learning_rate': [0.01, 0.05, 0.1],\n",
    "      'max_depth': [3, 5, 7]\n",
    "  }\n",
    "  \n",
    "  gbm = GradientBoostingClassifier()\n",
    "  grid_search = GridSearchCV(gbm, param_grid, cv=3, scoring='neg_log_loss')\n",
    "  grid_search.fit(X_train_flat, y_train.argmax(axis=1))\n",
    "  \n",
    "  print(f'Best parameters: {grid_search.best_params_}')\n",
    "  print(f'Best log loss: {-grid_search.best_score_}')\n",
    "  ```\n",
    "\n",
    "### **b. Learning Rate Scheduling**\n",
    "- **Dynamic Learning Rates:** Adjust the learning rate during training using schedulers like **StepLR**, **ReduceLROnPlateau**, or **CosineAnnealingLR**.\n",
    "  \n",
    "  ```python\n",
    "  scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10)\n",
    "  \n",
    "  # In your training loop\n",
    "  scheduler.step(average_loss)\n",
    "  ```\n",
    "\n",
    "### **c. Early Stopping**\n",
    "- **Prevent Overfitting:** Use early stopping based on validation loss to stop training when performance no longer improves.\n",
    "  \n",
    "  ```python\n",
    "  # Implement early stopping in your training loop\n",
    "  best_loss = float('inf')\n",
    "  patience = 10\n",
    "  trigger_times = 0\n",
    "  \n",
    "  for epoch in range(num_epochs):\n",
    "      # Training code...\n",
    "      \n",
    "      # Validation code\n",
    "      val_loss = ...  # Compute validation loss\n",
    "      if val_loss < best_loss:\n",
    "          best_loss = val_loss\n",
    "          trigger_times = 0\n",
    "          # Save the best model\n",
    "      else:\n",
    "          trigger_times += 1\n",
    "          if trigger_times >= patience:\n",
    "              print('Early stopping!')\n",
    "              break\n",
    "  ```\n",
    "\n",
    "### **d. Data Augmentation for Structured Data**\n",
    "- **Synthetic Feature Generation:** Create new samples by adding noise or perturbations to existing data points.\n",
    "  \n",
    "  ```python\n",
    "  noise = np.random.normal(0, 0.01, X_train.shape)\n",
    "  X_train_augmented = X_train + noise\n",
    "  y_train_augmented = y_train.copy()\n",
    "  \n",
    "  X_train = np.concatenate([X_train, X_train_augmented], axis=0)\n",
    "  y_train = np.concatenate([y_train, y_train_augmented], axis=0)\n",
    "  ```\n",
    "\n",
    "---\n",
    "\n",
    "## **5. Improve Evaluation Metrics and Validation**\n",
    "\n",
    "### **a. Use Appropriate Metrics**\n",
    "- **Task-Specific Metrics:** Ensure that the metrics align with your prediction goals. For probabilistic predictions, **Log Loss**, **Brier Score**, or **ROC-AUC** are suitable.\n",
    "  \n",
    "  ```python\n",
    "  from sklearn.metrics import log_loss, roc_auc_score\n",
    "  \n",
    "  logloss = log_loss(y_test, y_pred)\n",
    "  auc_score = roc_auc_score(y_test, y_pred, multi_class='ovr')\n",
    "  print(f'Log Loss: {logloss}, ROC-AUC: {auc_score}')\n",
    "  ```\n",
    "\n",
    "### **b. Cross-Validation**\n",
    "- **Robust Evaluation:** Use k-fold cross-validation to ensure that your model generalizes well across different subsets of data.\n",
    "  \n",
    "  ```python\n",
    "  from sklearn.model_selection import cross_val_score\n",
    "  from sklearn.ensemble import RandomForestClassifier\n",
    "  \n",
    "  clf = RandomForestClassifier()\n",
    "  scores = cross_val_score(clf, X_train_flat, y_train.argmax(axis=1), cv=5, scoring='neg_log_loss')\n",
    "  print(f'Cross-Validation Log Loss: {-scores.mean()}')\n",
    "  ```\n",
    "\n",
    "### **c. Validation Set**\n",
    "- **Hold-Out Set:** Split your training data into training and validation sets to monitor model performance during training.\n",
    "  \n",
    "  ```python\n",
    "  X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(\n",
    "      X_train_flat, y_train, test_size=0.2, random_state=42\n",
    "  )\n",
    "  ```\n",
    "\n",
    "---\n",
    "\n",
    "## **6. Address Model-Specific Challenges**\n",
    "\n",
    "### **a. Handle Imbalanced Data**\n",
    "- **Class Weights:** Assign higher weights to minority classes in the loss function.\n",
    "  \n",
    "  ```python\n",
    "  class_weights = compute_class_weight('balanced', classes=np.unique(y_train.argmax(axis=1)), y=y_train.argmax(axis=1))\n",
    "  class_weights = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
    "  \n",
    "  criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "  ```\n",
    "\n",
    "- **Oversampling/Undersampling:** Balance your dataset by oversampling minority classes or undersampling majority classes.\n",
    "\n",
    "### **b. Model Interpretability**\n",
    "- **Feature Importance:** Use tools like SHAP or LIME to understand which features are contributing most to predictions.\n",
    "  \n",
    "  ```python\n",
    "  import shap\n",
    "  \n",
    "  explainer = shap.TreeExplainer(gbm)\n",
    "  shap_values = explainer.shap_values(X_test_flat)\n",
    "  shap.summary_plot(shap_values, X_test_flat)\n",
    "  ```\n",
    "\n",
    "### **c. Ensemble Methods**\n",
    "- **Boosting:** As mentioned earlier, models like XGBoost or LightGBM can be powerful.\n",
    "- **Bagging:** Train multiple instances of your model on different subsets and average their predictions.\n",
    "- **Stacking:** Combine multiple models by stacking their predictions as inputs to a meta-model.\n",
    "\n",
    "---\n",
    "\n",
    "## **7. Optimize Neural Network-Specific Parameters**\n",
    "\n",
    "### **a. Increase Model Complexity**\n",
    "- **More Layers/Units:** Add more hidden layers or increase the number of units per layer.\n",
    "  \n",
    "  ```python\n",
    "  class RaceModel(nn.Module):\n",
    "      def __init__(self, input_size, hidden_size=256):\n",
    "          super(RaceModel, self).__init__()\n",
    "          self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "          self.relu = nn.ReLU()\n",
    "          self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "          self.relu = nn.ReLU()\n",
    "          self.fc3 = nn.Linear(hidden_size, 1)\n",
    "      \n",
    "      def forward(self, x, mask):\n",
    "          x = self.fc1(x)\n",
    "          x = self.relu(x)\n",
    "          x = self.fc2(x)\n",
    "          x = self.relu(x)\n",
    "          x = self.fc3(x)\n",
    "          x = x.view(x.size(0), -1)\n",
    "          scores = x.masked_fill(mask == 0, float('-inf'))\n",
    "          probs = torch.softmax(scores, dim=1)\n",
    "          return probs\n",
    "  ```\n",
    "\n",
    "### **b. Advanced Activation Functions**\n",
    "- **LeakyReLU or ELU:** These can help mitigate issues like dying neurons in ReLU.\n",
    "  \n",
    "  ```python\n",
    "  self.leaky_relu = nn.LeakyReLU(0.1)\n",
    "  \n",
    "  # In forward:\n",
    "  x = self.leaky_relu(self.fc1(x))\n",
    "  ```\n",
    "\n",
    "### **c. Regularization Techniques**\n",
    "- **Dropout Layers:** Prevent overfitting by randomly dropping units during training.\n",
    "  \n",
    "  ```python\n",
    "  self.dropout = nn.Dropout(p=0.5)\n",
    "  \n",
    "  # In forward:\n",
    "  x = self.dropout(self.relu(self.fc1(x)))\n",
    "  ```\n",
    "\n",
    "- **Batch Normalization:** Stabilize and accelerate training by normalizing layer inputs.\n",
    "  \n",
    "  ```python\n",
    "  self.batch_norm = nn.BatchNorm1d(hidden_size)\n",
    "  \n",
    "  # In forward:\n",
    "  x = self.batch_norm(x)\n",
    "  ```\n",
    "\n",
    "---\n",
    "\n",
    "## **8. Advanced Training Techniques**\n",
    "\n",
    "### **a. Transfer Learning**\n",
    "- **Pretrained Models:** Although more common in domains like image and text processing, investigate if there's a pretrained model relevant to your data that you can fine-tune.\n",
    "\n",
    "### **b. Curriculum Learning**\n",
    "- **Simpler First:** Train your model on easier examples first, then gradually increase the difficulty.\n",
    "\n",
    "### **c. Multi-Task Learning**\n",
    "- **Related Tasks:** If there are related tasks (e.g., predicting different aspects of the race), train the model to handle multiple tasks simultaneously. This can help the model learn more robust features.\n",
    "\n",
    "---\n",
    "\n",
    "## **9. Post-Training Enhancements**\n",
    "\n",
    "### **a. Model Calibration**\n",
    "- **Calibrate Probabilities:** Ensure that predicted probabilities reflect true likelihoods using techniques like Platt Scaling or Isotonic Regression.\n",
    "\n",
    "  ```python\n",
    "  from sklearn.calibration import CalibratedClassifierCV\n",
    "  \n",
    "  calibrated_clf = CalibratedClassifierCV(base_estimator=gbm, method='sigmoid', cv='prefit')\n",
    "  calibrated_clf.fit(X_val_split, y_val_split.argmax(axis=1))\n",
    "  y_pred_prob = calibrated_clf.predict_proba(X_test_flat)\n",
    "  ```\n",
    "\n",
    "### **b. Threshold Adjustment**\n",
    "- **Optimal Thresholds:** If your task involves classification, adjust decision thresholds to balance precision and recall based on your specific needs.\n",
    "\n",
    "---\n",
    "\n",
    "## **10. Implement Robust Evaluation and Monitoring**\n",
    "\n",
    "### **a. Detailed Evaluation Reports**\n",
    "- **Confusion Matrix:** Understand where your model is making errors.\n",
    "  \n",
    "  ```python\n",
    "  from sklearn.metrics import confusion_matrix\n",
    "  import seaborn as sns\n",
    "  import matplotlib.pyplot as plt\n",
    "  \n",
    "  y_pred_classes = y_pred.argmax(axis=1)\n",
    "  y_true = y_test.argmax(axis=1)\n",
    "  cm = confusion_matrix(y_true, y_pred_classes)\n",
    "  sns.heatmap(cm, annot=True, fmt='d')\n",
    "  plt.show()\n",
    "  ```\n",
    "\n",
    "- **Precision-Recall Curves:** Especially useful for imbalanced datasets.\n",
    "  \n",
    "  ```python\n",
    "  from sklearn.metrics import precision_recall_curve\n",
    "  \n",
    "  precision, recall, thresholds = precision_recall_curve(y_true, y_pred_prob[:,1])\n",
    "  plt.plot(recall, precision)\n",
    "  plt.xlabel('Recall')\n",
    "  plt.ylabel('Precision')\n",
    "  plt.show()\n",
    "  ```\n",
    "\n",
    "### **b. Cross-Validation Insights**\n",
    "- **Variance Across Folds:** Analyze if the model's performance is consistent across different data splits.\n",
    "\n",
    "### **c. Error Analysis**\n",
    "- **Misclassified Instances:** Inspect the cases where your model performs poorly to identify patterns or missing features.\n",
    "\n",
    "---\n",
    "\n",
    "## **11. Utilize Advanced Libraries and Tools**\n",
    "\n",
    "### **a. Automated Machine Learning (AutoML)**\n",
    "- **AutoML Tools:** Leverage tools like **AutoSklearn**, **TPOT**, or **H2O.ai** to automate the model selection and hyperparameter tuning process.\n",
    "\n",
    "### **b. Visualization Tools**\n",
    "- **TensorBoard or Weights & Biases:** Monitor training metrics, visualize model architecture, and track experiments.\n",
    "\n",
    "  ```python\n",
    "  from torch.utils.tensorboard import SummaryWriter\n",
    "  \n",
    "  writer = SummaryWriter()\n",
    "  \n",
    "  # In your training loop:\n",
    "  writer.add_scalar('Loss/train', loss.item(), epoch * len(train_loader) + batch_idx)\n",
    "  writer.close()\n",
    "  ```\n",
    "\n",
    "---\n",
    "\n",
    "## **12. Example: Applying Enhancements to Your Current Setup**\n",
    "\n",
    "Below is an example of how you can implement some of the above strategies within your existing code structure.\n",
    "\n",
    "### **a. Update Feature Engineering with Embeddings**\n",
    "\n",
    "Instead of one-hot encoding `rider_name`, use embeddings:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class RaceModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=256, num_rider_names=1000, rider_embedding_dim=50):\n",
    "        super(RaceModel, self).__init__()\n",
    "        self.rider_embedding = nn.Embedding(num_rider_names, rider_embedding_dim)\n",
    "        self.fc1 = nn.Linear(input_size + rider_embedding_dim, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc3 = nn.Linear(hidden_size, 1)\n",
    "    \n",
    "    def forward(self, x, rider_ids, mask):\n",
    "        # x: (batch_size, max_riders, num_features)\n",
    "        # rider_ids: (batch_size, max_riders)\n",
    "        embedded = self.rider_embedding(rider_ids)  # (batch_size, max_riders, embedding_dim)\n",
    "        x = torch.cat([x, embedded], dim=2)  # (batch_size, max_riders, num_features + embedding_dim)\n",
    "        batch_size, max_riders, _ = x.size()\n",
    "        x = x.view(-1, x.size(-1))\n",
    "        x = self.dropout(self.relu(self.fc1(x)))\n",
    "        x = self.dropout(self.relu(self.fc2(x)))\n",
    "        scores = self.fc3(x)\n",
    "        scores = scores.view(batch_size, max_riders)\n",
    "        scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "        probs = torch.softmax(scores, dim=1)\n",
    "        return probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### **b. Implement Hyperparameter Tuning with Optuna**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define the objective function\n",
    "def objective(trial):\n",
    "    hidden_size = trial.suggest_int('hidden_size', 128, 512)\n",
    "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
    "    dropout_rate = trial.suggest_uniform('dropout_rate', 0.2, 0.5)\n",
    "    \n",
    "    model = RaceModel(input_size, hidden_size=hidden_size, dropout_rate=dropout_rate).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.KLDivLoss(reduction='batchmean')\n",
    "    \n",
    "    # Training loop with a smaller number of epochs for tuning\n",
    "    for epoch in range(5):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for X_batch, y_batch, mask in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch, mask)\n",
    "            y_batch_norm = y_batch / y_batch.sum(dim=1, keepdim=True)\n",
    "            y_batch_norm = y_batch_norm * mask\n",
    "            loss = criterion(torch.log(outputs + 1e-8), y_batch_norm + 1e-8)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        average_loss = total_loss / len(train_loader)\n",
    "        trial.report(average_loss, epoch)\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "    \n",
    "    return average_loss\n",
    "\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "print(f'Best hyperparameters: {study.best_params}')\n",
    "print(f'Best loss: {study.best_value}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### **c. Leveraging Cross-Validation**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "all_losses = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X_train_flat)):\n",
    "    X_tr, X_val = X_train_flat[train_idx], X_train_flat[val_idx]\n",
    "    y_tr, y_val = y_train[train_idx], y_train[val_idx]\n",
    "    \n",
    "    # Create datasets and loaders\n",
    "    train_ds = RaceDataset(X_tr, y_tr)\n",
    "    val_ds = RaceDataset(X_val, y_val)\n",
    "    train_dl = DataLoader(train_ds, batch_size=16, shuffle=True)\n",
    "    val_dl = DataLoader(val_ds, batch_size=16, shuffle=False)\n",
    "    \n",
    "    # Initialize model, optimizer, and loss\n",
    "    model = RaceModel(input_size).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.KLDivLoss(reduction='batchmean')\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(10):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for X_batch, y_batch, mask in train_dl:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch, mask)\n",
    "            y_batch_norm = y_batch / y_batch.sum(dim=1, keepdim=True)\n",
    "            y_batch_norm = y_batch_norm * mask\n",
    "            loss = criterion(torch.log(outputs + 1e-8), y_batch_norm + 1e-8)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        average_loss = total_loss / len(train_dl)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch, mask in val_dl:\n",
    "                outputs = model(X_batch, mask)\n",
    "                y_batch_norm = y_batch / y_batch.sum(dim=1, keepdim=True)\n",
    "                y_batch_norm = y_batch_norm * mask\n",
    "                loss = criterion(torch.log(outputs + 1e-8), y_batch_norm + 1e-8)\n",
    "                val_loss += loss.item()\n",
    "        avg_val_loss = val_loss / len(val_dl)\n",
    "        all_losses.append(avg_val_loss)\n",
    "        print(f'Fold {fold+1}, Epoch {epoch+1}, Val Loss: {avg_val_loss:.4f}')\n",
    "\n",
    "print(f'Average Validation Loss across folds: {np.mean(all_losses):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "## **13. Additional Resources and Best Practices**\n",
    "\n",
    "### **a. Documentation and Tutorials**\n",
    "- **PyTorch Tutorials:** [https://pytorch.org/tutorials/](https://pytorch.org/tutorials/)\n",
    "- **Scikit-learn Documentation:** [https://scikit-learn.org/stable/documentation.html](https://scikit-learn.org/stable/documentation.html)\n",
    "\n",
    "### **b. Communities and Forums**\n",
    "- **PyTorch Forums:** [https://discuss.pytorch.org/](https://discuss.pytorch.org/)\n",
    "- **Kaggle:** Participate in competitions and discussions to learn best practices.\n",
    "- **Stack Overflow:** For specific coding issues and questions.\n",
    "\n",
    "### **c. Continuous Learning**\n",
    "- **Courses:** Consider taking advanced machine learning or deep learning courses to deepen your understanding.\n",
    "- **Books:** \"Deep Learning\" by Ian Goodfellow, \"Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow\" by Aurélien Géron.\n",
    "\n",
    "---\n",
    "\n",
    "## **14. Final Considerations**\n",
    "\n",
    "### **a. Iterative Improvement**\n",
    "- **Incremental Changes:** Implement one improvement at a time and monitor its impact. This helps in understanding what works best for your specific problem.\n",
    "- **Maintain a Baseline:** Always compare new models against a simple baseline to ensure that changes are beneficial.\n",
    "\n",
    "### **b. Model Interpretability**\n",
    "- **Understand Predictions:** Tools like **SHAP** or **LIME** can help you interpret model predictions and ensure that the model makes sense from a domain perspective.\n",
    "\n",
    "### **c. Deployment and Real-World Testing**\n",
    "- **Real-World Validation:** If possible, test your model in real-world scenarios to ensure it performs well outside of the training and testing datasets.\n",
    "- **Feedback Loop:** Incorporate feedback from real-world usage to continuously improve the model.\n",
    "\n",
    "---\n",
    "\n",
    "By systematically applying these strategies, you can significantly enhance your model's predictive performance. Start by identifying which areas (data quality, feature engineering, model complexity, etc.) are most likely to yield improvements in your specific context and prioritize efforts accordingly.\n",
    "\n",
    "Feel free to reach out with specific questions or if you need further guidance on any of the steps outlined above!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
