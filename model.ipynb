{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1168b13b0>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import mlflow.pytorch\n",
    "from mlflow.models.signature import infer_signature\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import optuna\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (2034, 207, 227)\n",
      "y_train shape: (2034, 207)\n",
      "X_test shape: (153, 207, 227)\n",
      "y_test shape: (153, 207)\n"
     ]
    }
   ],
   "source": [
    "# Load the data (adjust file paths as needed)\n",
    "X_train = np.load('X_train.npy', allow_pickle=True)\n",
    "y_train = np.load('y_train.npy', allow_pickle=True)\n",
    "X_test = np.load('X_test.npy', allow_pickle=True)\n",
    "y_test = np.load('y_test.npy', allow_pickle=True)\n",
    "\n",
    "print(f'X_train shape: {X_train.shape}')  # Expected: (num_races_train, max_riders, num_features)\n",
    "print(f'y_train shape: {y_train.shape}')  # Expected: (num_races_train, max_riders)\n",
    "print(f'X_test shape: {X_test.shape}')    # Expected: (num_races_test, max_riders, num_features)\n",
    "print(f'y_test shape: {y_test.shape}')    # Expected: (num_races_test, max_riders)\n",
    "\n",
    "# Flatten the data for scikit-learn models\n",
    "X_train_flat = X_train.reshape(-1, X_train.shape[2])    # Shape: (num_races_train * max_riders, num_features)\n",
    "X_test_flat = X_test.reshape(-1, X_test.shape[2])       # Shape: (num_races_test * max_riders, num_features)\n",
    "\n",
    "# Flatten the targets\n",
    "y_train_flat = y_train.flatten()  # Shape: (num_races_train * max_riders,)\n",
    "y_test_flat = y_test.flatten()    # Shape: (num_races_test * max_riders,)\n",
    "\n",
    "# Filter out invalid targets (if necessary)\n",
    "valid_indices_train = y_train_flat > 0\n",
    "valid_indices_test = y_test_flat > 0\n",
    "\n",
    "X_train_flat = X_train_flat[valid_indices_train]\n",
    "y_train_flat = y_train_flat[valid_indices_train]\n",
    "\n",
    "X_test_flat = X_test_flat[valid_indices_test]\n",
    "y_test_flat = y_test_flat[valid_indices_test]\n",
    "\n",
    "# # Optionally scale the features\n",
    "# scaler = StandardScaler()\n",
    "# X_train_flat = scaler.fit_transform(X_train_flat)\n",
    "# X_test_flat = scaler.transform(X_test_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='file:///Users/feliks/Documents/Faks/Diplomska/App/mlruns/586264776644289656', creation_time=1732972027382, experiment_id='586264776644289656', last_update_time=1732972027382, lifecycle_stage='active', name='Race_Prediction_Experiment_II', tags={}>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set MLflow experiment\n",
    "mlflow.set_tracking_uri(\"file:./mlruns\")\n",
    "mlflow.set_experiment(\"Race_Prediction_Experiment_II\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_model(model_class, param_grid, model_name, X_train, y_train, X_test, y_test):\n",
    "    from itertools import product\n",
    "    import pandas as pd\n",
    "\n",
    "    # Generate all combinations of hyperparameters\n",
    "    keys = param_grid.keys()\n",
    "    values = (param_grid[key] for key in keys)\n",
    "    param_combinations = [dict(zip(keys, combination)) for combination in product(*values)]\n",
    "\n",
    "    # For each combination, train and log the model\n",
    "    for idx, params in enumerate(param_combinations):\n",
    "        # Initialize model with current hyperparameters\n",
    "        model = model_class(**params)\n",
    "\n",
    "        # Fit the model\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Predict on test set\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        # Calculate metrics\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        mape = mean_absolute_percentage_error(y_test, y_pred)\n",
    "        rmse = np.sqrt(mse)\n",
    "        smape = symmetric_mean_absolute_percentage_error(y_test, y_pred)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "        # Start MLflow run\n",
    "        with mlflow.start_run(run_name=f\"{model_name} - Run {idx+1}\"):\n",
    "            # Log parameters\n",
    "            mlflow.log_param(\"model_class\", model_name)\n",
    "            mlflow.log_params(params)\n",
    "\n",
    "            # Log metrics\n",
    "            mlflow.log_metric(\"test_mse\", mse)\n",
    "            mlflow.log_metric(\"test_mae\", mae)\n",
    "            mlflow.log_metric(\"test_r2\", r2)\n",
    "            mlflow.log_metric(\"test_mape\", mape)\n",
    "            mlflow.log_metric(\"test_rmse\", rmse)\n",
    "            mlflow.log_metric(\"test_smape\", smape)\n",
    "\n",
    "            # Log the model\n",
    "            input_example = X_train[:5]\n",
    "            signature = infer_signature(X_train, model.predict(X_train))\n",
    "\n",
    "            mlflow.sklearn.log_model(\n",
    "                sk_model=model,\n",
    "                artifact_path=\"model\",\n",
    "                input_example=input_example,\n",
    "                signature=signature\n",
    "            )\n",
    "\n",
    "            # Print results\n",
    "            print(f\"\\n{model_name} Run {idx+1} parameters: {params}\")\n",
    "            print(f\"{model_name} Run {idx+1} Test MSE: {mse:.4f}\")\n",
    "            print(f\"{model_name} Run {idx+1} Test MAE: {mae:.4f}\")\n",
    "            print(f\"{model_name} Run {idx+1} Test R^2 Score: {r2:.4f}\")\n",
    "            print(f\"{model_name} Run {idx+1} Test MAPE: {mape:.4f}\")\n",
    "            print(f\"{model_name} Run {idx+1} Test RMSE: {rmse:.4f}\")\n",
    "            print(f\"{model_name} Run {idx+1} Test sMAPE: {smape:.4f}\")\n",
    "\n",
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    epsilon = 1e-8  # Small number to prevent division by zero\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    # Avoid division by zero\n",
    "    mask = np.abs(y_true) > epsilon\n",
    "    if np.sum(mask) == 0:\n",
    "        return np.inf  # Return infinity if no valid entries\n",
    "    mape = np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100\n",
    "    return mape\n",
    "\n",
    "def symmetric_mean_absolute_percentage_error(y_true, y_pred):\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    denominator = (np.abs(y_true) + np.abs(y_pred)) / 2\n",
    "    diff = np.abs(y_pred - y_true)\n",
    "    # Avoid division by zero\n",
    "    mask = denominator != 0\n",
    "    smape = np.mean((diff[mask] / denominator[mask])) * 100\n",
    "    return smape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(459, 2) (459, 2)\n",
      "\n",
      "Linear Regression Run 1 parameters: {}\n",
      "Linear Regression Run 1 Test MSE: 0.4521\n",
      "Linear Regression Run 1 Test MAE: 0.6713\n",
      "Linear Regression Run 1 Test R^2 Score: 0.0000\n",
      "Linear Regression Run 1 Test MAPE: 67.1267\n",
      "Linear Regression Run 1 Test RMSE: 0.6724\n",
      "Linear Regression Run 1 Test sMAPE: 101.3003\n",
      "Linear Regression Run 1 Test Log Loss: 0.0000\n",
      "Linear Regression Run 1 Test Brier Loss: 0.0000\n"
     ]
    }
   ],
   "source": [
    "linear_reg = LinearRegression\n",
    "param_grid_lr = {\n",
    "    # No hyperparameters to tune\n",
    "}\n",
    "\n",
    "train_and_evaluate_model(\n",
    "    model_class=linear_reg,\n",
    "    param_grid=param_grid_lr,\n",
    "    model_name=\"Linear Regression\",\n",
    "    X_train=X_train_flat,\n",
    "    y_train=y_train_flat,\n",
    "    X_test=X_test_flat,\n",
    "    y_test=y_test_flat\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(459, 2) (459, 2)\n",
      "\n",
      "Ridge Regression Run 1 parameters: {'alpha': 0.1}\n",
      "Ridge Regression Run 1 Test MSE: 0.4522\n",
      "Ridge Regression Run 1 Test MAE: 0.6713\n",
      "Ridge Regression Run 1 Test R^2 Score: 0.0000\n",
      "Ridge Regression Run 1 Test MAPE: 67.1273\n",
      "Ridge Regression Run 1 Test RMSE: 0.6724\n",
      "Ridge Regression Run 1 Test sMAPE: 101.3007\n",
      "Ridge Regression Run 1 Test Log Loss: 0.0000\n",
      "Ridge Regression Run 1 Test Brier Loss: 0.0000\n",
      "(459, 2) (459, 2)\n",
      "\n",
      "Ridge Regression Run 2 parameters: {'alpha': 0.9}\n",
      "Ridge Regression Run 2 Test MSE: 0.4522\n",
      "Ridge Regression Run 2 Test MAE: 0.6714\n",
      "Ridge Regression Run 2 Test R^2 Score: 0.0000\n",
      "Ridge Regression Run 2 Test MAPE: 67.1364\n",
      "Ridge Regression Run 2 Test RMSE: 0.6725\n",
      "Ridge Regression Run 2 Test sMAPE: 101.3137\n",
      "Ridge Regression Run 2 Test Log Loss: 0.0000\n",
      "Ridge Regression Run 2 Test Brier Loss: 0.0000\n",
      "(459, 2) (459, 2)\n",
      "\n",
      "Ridge Regression Run 3 parameters: {'alpha': 1.0}\n",
      "Ridge Regression Run 3 Test MSE: 0.4522\n",
      "Ridge Regression Run 3 Test MAE: 0.6714\n",
      "Ridge Regression Run 3 Test R^2 Score: 0.0000\n",
      "Ridge Regression Run 3 Test MAPE: 67.1375\n",
      "Ridge Regression Run 3 Test RMSE: 0.6725\n",
      "Ridge Regression Run 3 Test sMAPE: 101.3154\n",
      "Ridge Regression Run 3 Test Log Loss: 0.0000\n",
      "Ridge Regression Run 3 Test Brier Loss: 0.0000\n",
      "(459, 2) (459, 2)\n",
      "\n",
      "Ridge Regression Run 4 parameters: {'alpha': 1.5}\n",
      "Ridge Regression Run 4 Test MSE: 0.4523\n",
      "Ridge Regression Run 4 Test MAE: 0.6714\n",
      "Ridge Regression Run 4 Test R^2 Score: 0.0000\n",
      "Ridge Regression Run 4 Test MAPE: 67.1432\n",
      "Ridge Regression Run 4 Test RMSE: 0.6725\n",
      "Ridge Regression Run 4 Test sMAPE: 101.3243\n",
      "Ridge Regression Run 4 Test Log Loss: 0.0000\n",
      "Ridge Regression Run 4 Test Brier Loss: 0.0000\n",
      "(459, 2) (459, 2)\n",
      "\n",
      "Ridge Regression Run 5 parameters: {'alpha': 2.0}\n",
      "Ridge Regression Run 5 Test MSE: 0.4523\n",
      "Ridge Regression Run 5 Test MAE: 0.6715\n",
      "Ridge Regression Run 5 Test R^2 Score: 0.0000\n",
      "Ridge Regression Run 5 Test MAPE: 67.1487\n",
      "Ridge Regression Run 5 Test RMSE: 0.6726\n",
      "Ridge Regression Run 5 Test sMAPE: 101.3332\n",
      "Ridge Regression Run 5 Test Log Loss: 0.0000\n",
      "Ridge Regression Run 5 Test Brier Loss: 0.0000\n",
      "(459, 2) (459, 2)\n",
      "\n",
      "Ridge Regression Run 6 parameters: {'alpha': 10.0}\n",
      "Ridge Regression Run 6 Test MSE: 0.4530\n",
      "Ridge Regression Run 6 Test MAE: 0.6721\n",
      "Ridge Regression Run 6 Test R^2 Score: 0.0000\n",
      "Ridge Regression Run 6 Test MAPE: 67.2134\n",
      "Ridge Regression Run 6 Test RMSE: 0.6731\n",
      "Ridge Regression Run 6 Test sMAPE: 101.4463\n",
      "Ridge Regression Run 6 Test Log Loss: 0.0000\n",
      "Ridge Regression Run 6 Test Brier Loss: 0.0000\n"
     ]
    }
   ],
   "source": [
    "ridge_reg = Ridge\n",
    "param_grid_ridge = {\n",
    "    'alpha': [0.1, 0.9, 1.0, 1.5, 2.0, 10.0]\n",
    "}\n",
    "\n",
    "train_and_evaluate_model(\n",
    "    model_class=ridge_reg,\n",
    "    param_grid=param_grid_ridge,\n",
    "    model_name=\"Ridge Regression\",\n",
    "    X_train=X_train_flat,\n",
    "    y_train=y_train_flat,\n",
    "    X_test=X_test_flat,\n",
    "    y_test=y_test_flat\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Lasso Regression Run 1 parameters: {'alpha': 0.0001, 'max_iter': 10000}\n",
      "Lasso Regression Run 1 Test MSE: 0.0563\n",
      "Lasso Regression Run 1 Test MAE: 0.2112\n",
      "Lasso Regression Run 1 Test R^2 Score: 0.0410\n",
      "Lasso Regression Run 1 Test MAPE: 111.9876\n",
      "Lasso Regression Run 1 Test RMSE: 0.2372\n",
      "Lasso Regression Run 1 Test sMAPE: 67.8755\n",
      "\n",
      "Lasso Regression Run 2 parameters: {'alpha': 0.001, 'max_iter': 10000}\n",
      "Lasso Regression Run 2 Test MSE: 0.0572\n",
      "Lasso Regression Run 2 Test MAE: 0.2150\n",
      "Lasso Regression Run 2 Test R^2 Score: 0.0260\n",
      "Lasso Regression Run 2 Test MAPE: 115.0582\n",
      "Lasso Regression Run 2 Test RMSE: 0.2391\n",
      "Lasso Regression Run 2 Test sMAPE: 68.8978\n",
      "\n",
      "Lasso Regression Run 3 parameters: {'alpha': 0.01, 'max_iter': 10000}\n",
      "Lasso Regression Run 3 Test MSE: 0.0583\n",
      "Lasso Regression Run 3 Test MAE: 0.2199\n",
      "Lasso Regression Run 3 Test R^2 Score: 0.0060\n",
      "Lasso Regression Run 3 Test MAPE: 118.7917\n",
      "Lasso Regression Run 3 Test RMSE: 0.2415\n",
      "Lasso Regression Run 3 Test sMAPE: 70.2226\n",
      "\n",
      "Lasso Regression Run 4 parameters: {'alpha': 0.1, 'max_iter': 10000}\n",
      "Lasso Regression Run 4 Test MSE: 0.0582\n",
      "Lasso Regression Run 4 Test MAE: 0.2196\n",
      "Lasso Regression Run 4 Test R^2 Score: 0.0079\n",
      "Lasso Regression Run 4 Test MAPE: 118.4334\n",
      "Lasso Regression Run 4 Test RMSE: 0.2413\n",
      "Lasso Regression Run 4 Test sMAPE: 70.1490\n",
      "\n",
      "Lasso Regression Run 5 parameters: {'alpha': 1.0, 'max_iter': 10000}\n",
      "Lasso Regression Run 5 Test MSE: 0.0581\n",
      "Lasso Regression Run 5 Test MAE: 0.2190\n",
      "Lasso Regression Run 5 Test R^2 Score: 0.0102\n",
      "Lasso Regression Run 5 Test MAPE: 117.7239\n",
      "Lasso Regression Run 5 Test RMSE: 0.2410\n",
      "Lasso Regression Run 5 Test sMAPE: 70.0111\n",
      "\n",
      "Lasso Regression Run 6 parameters: {'alpha': 1.5, 'max_iter': 10000}\n",
      "Lasso Regression Run 6 Test MSE: 0.0581\n",
      "Lasso Regression Run 6 Test MAE: 0.2191\n",
      "Lasso Regression Run 6 Test R^2 Score: 0.0096\n",
      "Lasso Regression Run 6 Test MAPE: 117.7553\n",
      "Lasso Regression Run 6 Test RMSE: 0.2411\n",
      "Lasso Regression Run 6 Test sMAPE: 70.0298\n",
      "\n",
      "Lasso Regression Run 7 parameters: {'alpha': 2.0, 'max_iter': 10000}\n",
      "Lasso Regression Run 7 Test MSE: 0.0582\n",
      "Lasso Regression Run 7 Test MAE: 0.2191\n",
      "Lasso Regression Run 7 Test R^2 Score: 0.0089\n",
      "Lasso Regression Run 7 Test MAPE: 117.7867\n",
      "Lasso Regression Run 7 Test RMSE: 0.2412\n",
      "Lasso Regression Run 7 Test sMAPE: 70.0484\n",
      "\n",
      "Lasso Regression Run 8 parameters: {'alpha': 10.0, 'max_iter': 10000}\n",
      "Lasso Regression Run 8 Test MSE: 0.0587\n",
      "Lasso Regression Run 8 Test MAE: 0.2199\n",
      "Lasso Regression Run 8 Test R^2 Score: -0.0000\n",
      "Lasso Regression Run 8 Test MAPE: 118.1538\n",
      "Lasso Regression Run 8 Test RMSE: 0.2422\n",
      "Lasso Regression Run 8 Test sMAPE: 70.2532\n"
     ]
    }
   ],
   "source": [
    "lasso_reg = Lasso\n",
    "param_grid_lasso = {\n",
    "    'alpha': [0.0001, 0.001, 0.01, 0.1, 1.0, 1.5, 2.0, 10.0],\n",
    "    'max_iter': [10000]\n",
    "}\n",
    "\n",
    "train_and_evaluate_model(\n",
    "    model_class=lasso_reg,\n",
    "    param_grid=param_grid_lasso,\n",
    "    model_name=\"Lasso Regression\",\n",
    "    X_train=X_train_flat,\n",
    "    y_train=y_train_flat,\n",
    "    X_test=X_test_flat,\n",
    "    y_test=y_test_flat\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Decision Tree Regressor Run 1 parameters: {'max_depth': None, 'min_samples_split': 2}\n",
      "Decision Tree Regressor Run 1 Test MSE: 0.1108\n",
      "Decision Tree Regressor Run 1 Test MAE: 0.2423\n",
      "Decision Tree Regressor Run 1 Test R^2 Score: -0.8878\n",
      "Decision Tree Regressor Run 1 Test MAPE: 127.2581\n",
      "Decision Tree Regressor Run 1 Test RMSE: 0.3328\n",
      "Decision Tree Regressor Run 1 Test sMAPE: 73.8828\n",
      "\n",
      "Decision Tree Regressor Run 2 parameters: {'max_depth': None, 'min_samples_split': 5}\n",
      "Decision Tree Regressor Run 2 Test MSE: 0.1087\n",
      "Decision Tree Regressor Run 2 Test MAE: 0.2436\n",
      "Decision Tree Regressor Run 2 Test R^2 Score: -0.8524\n",
      "Decision Tree Regressor Run 2 Test MAPE: 128.2310\n",
      "Decision Tree Regressor Run 2 Test RMSE: 0.3297\n",
      "Decision Tree Regressor Run 2 Test sMAPE: 73.6349\n",
      "\n",
      "Decision Tree Regressor Run 3 parameters: {'max_depth': None, 'min_samples_split': 10}\n",
      "Decision Tree Regressor Run 3 Test MSE: 0.0919\n",
      "Decision Tree Regressor Run 3 Test MAE: 0.2307\n",
      "Decision Tree Regressor Run 3 Test R^2 Score: -0.5656\n",
      "Decision Tree Regressor Run 3 Test MAPE: 118.8043\n",
      "Decision Tree Regressor Run 3 Test RMSE: 0.3031\n",
      "Decision Tree Regressor Run 3 Test sMAPE: 71.8612\n",
      "\n",
      "Decision Tree Regressor Run 4 parameters: {'max_depth': 5, 'min_samples_split': 2}\n",
      "Decision Tree Regressor Run 4 Test MSE: 0.0613\n",
      "Decision Tree Regressor Run 4 Test MAE: 0.2164\n",
      "Decision Tree Regressor Run 4 Test R^2 Score: -0.0448\n",
      "Decision Tree Regressor Run 4 Test MAPE: 113.3211\n",
      "Decision Tree Regressor Run 4 Test RMSE: 0.2476\n",
      "Decision Tree Regressor Run 4 Test sMAPE: 68.8924\n",
      "\n",
      "Decision Tree Regressor Run 5 parameters: {'max_depth': 5, 'min_samples_split': 5}\n",
      "Decision Tree Regressor Run 5 Test MSE: 0.0613\n",
      "Decision Tree Regressor Run 5 Test MAE: 0.2164\n",
      "Decision Tree Regressor Run 5 Test R^2 Score: -0.0448\n",
      "Decision Tree Regressor Run 5 Test MAPE: 113.3211\n",
      "Decision Tree Regressor Run 5 Test RMSE: 0.2476\n",
      "Decision Tree Regressor Run 5 Test sMAPE: 68.8924\n",
      "\n",
      "Decision Tree Regressor Run 6 parameters: {'max_depth': 5, 'min_samples_split': 10}\n",
      "Decision Tree Regressor Run 6 Test MSE: 0.0611\n",
      "Decision Tree Regressor Run 6 Test MAE: 0.2161\n",
      "Decision Tree Regressor Run 6 Test R^2 Score: -0.0409\n",
      "Decision Tree Regressor Run 6 Test MAPE: 113.2190\n",
      "Decision Tree Regressor Run 6 Test RMSE: 0.2471\n",
      "Decision Tree Regressor Run 6 Test sMAPE: 68.8651\n",
      "\n",
      "Decision Tree Regressor Run 7 parameters: {'max_depth': 10, 'min_samples_split': 2}\n",
      "Decision Tree Regressor Run 7 Test MSE: 0.0721\n",
      "Decision Tree Regressor Run 7 Test MAE: 0.2215\n",
      "Decision Tree Regressor Run 7 Test R^2 Score: -0.2285\n",
      "Decision Tree Regressor Run 7 Test MAPE: 115.6899\n",
      "Decision Tree Regressor Run 7 Test RMSE: 0.2685\n",
      "Decision Tree Regressor Run 7 Test sMAPE: 69.8111\n",
      "\n",
      "Decision Tree Regressor Run 8 parameters: {'max_depth': 10, 'min_samples_split': 5}\n",
      "Decision Tree Regressor Run 8 Test MSE: 0.0716\n",
      "Decision Tree Regressor Run 8 Test MAE: 0.2206\n",
      "Decision Tree Regressor Run 8 Test R^2 Score: -0.2196\n",
      "Decision Tree Regressor Run 8 Test MAPE: 115.1425\n",
      "Decision Tree Regressor Run 8 Test RMSE: 0.2675\n",
      "Decision Tree Regressor Run 8 Test sMAPE: 69.2303\n",
      "\n",
      "Decision Tree Regressor Run 9 parameters: {'max_depth': 10, 'min_samples_split': 10}\n",
      "Decision Tree Regressor Run 9 Test MSE: 0.0682\n",
      "Decision Tree Regressor Run 9 Test MAE: 0.2166\n",
      "Decision Tree Regressor Run 9 Test R^2 Score: -0.1623\n",
      "Decision Tree Regressor Run 9 Test MAPE: 113.2870\n",
      "Decision Tree Regressor Run 9 Test RMSE: 0.2612\n",
      "Decision Tree Regressor Run 9 Test sMAPE: 68.6276\n",
      "\n",
      "Decision Tree Regressor Run 10 parameters: {'max_depth': 20, 'min_samples_split': 2}\n",
      "Decision Tree Regressor Run 10 Test MSE: 0.1026\n",
      "Decision Tree Regressor Run 10 Test MAE: 0.2364\n",
      "Decision Tree Regressor Run 10 Test R^2 Score: -0.7479\n",
      "Decision Tree Regressor Run 10 Test MAPE: 123.3912\n",
      "Decision Tree Regressor Run 10 Test RMSE: 0.3203\n",
      "Decision Tree Regressor Run 10 Test sMAPE: 72.5231\n",
      "\n",
      "Decision Tree Regressor Run 11 parameters: {'max_depth': 20, 'min_samples_split': 5}\n",
      "Decision Tree Regressor Run 11 Test MSE: 0.1027\n",
      "Decision Tree Regressor Run 11 Test MAE: 0.2394\n",
      "Decision Tree Regressor Run 11 Test R^2 Score: -0.7501\n",
      "Decision Tree Regressor Run 11 Test MAPE: 122.7318\n",
      "Decision Tree Regressor Run 11 Test RMSE: 0.3205\n",
      "Decision Tree Regressor Run 11 Test sMAPE: 72.3642\n",
      "\n",
      "Decision Tree Regressor Run 12 parameters: {'max_depth': 20, 'min_samples_split': 10}\n",
      "Decision Tree Regressor Run 12 Test MSE: 0.0908\n",
      "Decision Tree Regressor Run 12 Test MAE: 0.2302\n",
      "Decision Tree Regressor Run 12 Test R^2 Score: -0.5474\n",
      "Decision Tree Regressor Run 12 Test MAPE: 118.5458\n",
      "Decision Tree Regressor Run 12 Test RMSE: 0.3013\n",
      "Decision Tree Regressor Run 12 Test sMAPE: 71.4473\n"
     ]
    }
   ],
   "source": [
    "decision_tree_reg = DecisionTreeRegressor\n",
    "param_grid_dtree_reg = {\n",
    "    'max_depth': [None, 5, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "train_and_evaluate_model(\n",
    "    model_class=decision_tree_reg,\n",
    "    param_grid=param_grid_dtree_reg,\n",
    "    model_name=\"Decision Tree Regressor\",\n",
    "    X_train=X_train_flat,\n",
    "    y_train=y_train_flat,\n",
    "    X_test=X_test_flat,\n",
    "    y_test=y_test_flat\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svr_model_class = SVR\n",
    "param_grid_svr = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'kernel': ['linear', 'rbf'],\n",
    "    'epsilon': [0.01, 0.1, 1]\n",
    "}\n",
    "\n",
    "train_and_evaluate_model(\n",
    "    model_class=svr_model_class,\n",
    "    param_grid=param_grid_svr,\n",
    "    model_name=\"Support Vector Regressor\",\n",
    "    X_train=X_train_flat,\n",
    "    y_train=y_train_flat,\n",
    "    X_test=X_test_flat,\n",
    "    y_test=y_test_flat\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_reg = RandomForestRegressor()\n",
    "param_grid_rf_reg = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [None, 5, 10],\n",
    "    'min_samples_split': [2, 5]\n",
    "}\n",
    "\n",
    "train_and_evaluate_model(\n",
    "    model=random_forest_reg,\n",
    "    param_grid=param_grid_rf_reg,\n",
    "    model_name=\"Random Forest Regressor\",\n",
    "    X_train=X_train_flat,\n",
    "    y_train=y_train_flat,\n",
    "    X_test=X_test_flat,\n",
    "    y_test=y_test_flat\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n"
     ]
    }
   ],
   "source": [
    "gb_regressor = GradientBoostingRegressor()\n",
    "param_grid_gb_reg = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'learning_rate': [0.01, 0.1],\n",
    "    'max_depth': [3, 5]\n",
    "}\n",
    "\n",
    "train_and_evaluate_model(\n",
    "    model=gb_regressor,\n",
    "    param_grid=param_grid_gb_reg,\n",
    "    model_name=\"Gradient Boosting Regressor\",\n",
    "    X_train=X_train_flat,\n",
    "    y_train=y_train_flat,\n",
    "    X_test=X_test_flat,\n",
    "    y_test=y_test_flat\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "XGBoost Regressor Run 1 parameters: {'n_estimators': 100, 'learning_rate': 0.01, 'max_depth': 3, 'objective': 'reg:squarederror'}\n",
      "XGBoost Regressor Run 1 Test MSE: 0.0572\n",
      "XGBoost Regressor Run 1 Test MAE: 0.2148\n",
      "XGBoost Regressor Run 1 Test R^2 Score: 0.0259\n",
      "XGBoost Regressor Run 1 Test MAPE: 113.9762\n",
      "XGBoost Regressor Run 1 Test RMSE: 0.2391\n",
      "XGBoost Regressor Run 1 Test sMAPE: 68.9682\n",
      "\n",
      "XGBoost Regressor Run 2 parameters: {'n_estimators': 100, 'learning_rate': 0.01, 'max_depth': 5, 'objective': 'reg:squarederror'}\n",
      "XGBoost Regressor Run 2 Test MSE: 0.0571\n",
      "XGBoost Regressor Run 2 Test MAE: 0.2140\n",
      "XGBoost Regressor Run 2 Test R^2 Score: 0.0275\n",
      "XGBoost Regressor Run 2 Test MAPE: 113.7460\n",
      "XGBoost Regressor Run 2 Test RMSE: 0.2389\n",
      "XGBoost Regressor Run 2 Test sMAPE: 68.6829\n",
      "\n",
      "XGBoost Regressor Run 3 parameters: {'n_estimators': 100, 'learning_rate': 0.1, 'max_depth': 3, 'objective': 'reg:squarederror'}\n",
      "XGBoost Regressor Run 3 Test MSE: 0.0560\n",
      "XGBoost Regressor Run 3 Test MAE: 0.2047\n",
      "XGBoost Regressor Run 3 Test R^2 Score: 0.0455\n",
      "XGBoost Regressor Run 3 Test MAPE: 107.1268\n",
      "XGBoost Regressor Run 3 Test RMSE: 0.2367\n",
      "XGBoost Regressor Run 3 Test sMAPE: 66.0860\n",
      "\n",
      "XGBoost Regressor Run 4 parameters: {'n_estimators': 100, 'learning_rate': 0.1, 'max_depth': 5, 'objective': 'reg:squarederror'}\n",
      "XGBoost Regressor Run 4 Test MSE: 0.0568\n",
      "XGBoost Regressor Run 4 Test MAE: 0.2033\n",
      "XGBoost Regressor Run 4 Test R^2 Score: 0.0328\n",
      "XGBoost Regressor Run 4 Test MAPE: 106.1542\n",
      "XGBoost Regressor Run 4 Test RMSE: 0.2382\n",
      "XGBoost Regressor Run 4 Test sMAPE: 65.6255\n",
      "\n",
      "XGBoost Regressor Run 5 parameters: {'n_estimators': 200, 'learning_rate': 0.01, 'max_depth': 3, 'objective': 'reg:squarederror'}\n",
      "XGBoost Regressor Run 5 Test MSE: 0.0561\n",
      "XGBoost Regressor Run 5 Test MAE: 0.2114\n",
      "XGBoost Regressor Run 5 Test R^2 Score: 0.0436\n",
      "XGBoost Regressor Run 5 Test MAPE: 111.9025\n",
      "XGBoost Regressor Run 5 Test RMSE: 0.2369\n",
      "XGBoost Regressor Run 5 Test sMAPE: 68.0615\n",
      "\n",
      "XGBoost Regressor Run 6 parameters: {'n_estimators': 200, 'learning_rate': 0.01, 'max_depth': 5, 'objective': 'reg:squarederror'}\n",
      "XGBoost Regressor Run 6 Test MSE: 0.0557\n",
      "XGBoost Regressor Run 6 Test MAE: 0.2087\n",
      "XGBoost Regressor Run 6 Test R^2 Score: 0.0504\n",
      "XGBoost Regressor Run 6 Test MAPE: 111.1046\n",
      "XGBoost Regressor Run 6 Test RMSE: 0.2361\n",
      "XGBoost Regressor Run 6 Test sMAPE: 67.2705\n",
      "\n",
      "XGBoost Regressor Run 7 parameters: {'n_estimators': 200, 'learning_rate': 0.1, 'max_depth': 3, 'objective': 'reg:squarederror'}\n",
      "XGBoost Regressor Run 7 Test MSE: 0.0568\n",
      "XGBoost Regressor Run 7 Test MAE: 0.2032\n",
      "XGBoost Regressor Run 7 Test R^2 Score: 0.0316\n",
      "XGBoost Regressor Run 7 Test MAPE: 105.4697\n",
      "XGBoost Regressor Run 7 Test RMSE: 0.2384\n",
      "XGBoost Regressor Run 7 Test sMAPE: 65.5679\n",
      "\n",
      "XGBoost Regressor Run 8 parameters: {'n_estimators': 200, 'learning_rate': 0.1, 'max_depth': 5, 'objective': 'reg:squarederror'}\n",
      "XGBoost Regressor Run 8 Test MSE: 0.0579\n",
      "XGBoost Regressor Run 8 Test MAE: 0.2036\n",
      "XGBoost Regressor Run 8 Test R^2 Score: 0.0136\n",
      "XGBoost Regressor Run 8 Test MAPE: 106.7382\n",
      "XGBoost Regressor Run 8 Test RMSE: 0.2406\n",
      "XGBoost Regressor Run 8 Test sMAPE: 65.5570\n"
     ]
    }
   ],
   "source": [
    "xgboost_reg = xgb.XGBRegressor\n",
    "param_grid_xgb_reg = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'learning_rate': [0.01, 0.1],\n",
    "    'max_depth': [3, 5],\n",
    "    'objective': ['reg:squarederror']\n",
    "}\n",
    "\n",
    "train_and_evaluate_model(\n",
    "    model_class=xgboost_reg,\n",
    "    param_grid=param_grid_xgb_reg,\n",
    "    model_name=\"XGBoost Regressor\",\n",
    "    X_train=X_train_flat,\n",
    "    y_train=y_train_flat,\n",
    "    X_test=X_test_flat,\n",
    "    y_test=y_test_flat\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.251038 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3654\n",
      "[LightGBM] [Info] Number of data points in the train set: 219147, number of used features: 145\n",
      "[LightGBM] [Info] Start training from score 0.006051\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.086654 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3654\n",
      "[LightGBM] [Info] Number of data points in the train set: 219147, number of used features: 145\n",
      "[LightGBM] [Info] Start training from score 0.006051\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.202939 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3654\n",
      "[LightGBM] [Info] Number of data points in the train set: 219147, number of used features: 145\n",
      "[LightGBM] [Info] Start training from score 0.006051\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.041435 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3701\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.042858 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3701\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.031709 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3714\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019379 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Number of data points in the train set: 219146, number of used features: 166\n",
      "[LightGBM] [Info] Total Bins 3701\n",
      "[LightGBM] [Info] Number of data points in the train set: 219146, number of used features: 166\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.021993 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3701\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.070045 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3714\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.048020 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Number of data points in the train set: 219147, number of used features: 167\n",
      "[LightGBM] [Info] Total Bins 3714\n",
      "[LightGBM] [Info] Number of data points in the train set: 219146, number of used features: 166\n",
      "[LightGBM] [Info] Number of data points in the train set: 219147, number of used features: 167\n",
      "[LightGBM] [Info] Number of data points in the train set: 219146, number of used features: 166\n",
      "[LightGBM] [Info] Number of data points in the train set: 219147, number of used features: 167\n",
      "[LightGBM] [Info] Start training from score 0.006283\n",
      "[LightGBM] [Info] Start training from score 0.006283\n",
      "[LightGBM] [Info] Start training from score 0.006229\n",
      "[LightGBM] [Info] Start training from score 0.006283\n",
      "[LightGBM] [Info] Start training from score 0.006283\n",
      "[LightGBM] [Info] Start training from score 0.006229\n",
      "[LightGBM] [Info] Start training from score 0.006229\n",
      "[CV 2/3] END learning_rate=0.01, n_estimators=100, num_leaves=31;, score=-0.002 total time=  15.5s\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012872 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3714\n",
      "[LightGBM] [Info] Number of data points in the train set: 219147, number of used features: 167\n",
      "[LightGBM] [Info] Start training from score 0.006229\n",
      "[CV 1/3] END learning_rate=0.01, n_estimators=100, num_leaves=31;, score=-0.002 total time=  17.3s\n",
      "[CV 3/3] END learning_rate=0.01, n_estimators=100, num_leaves=31;, score=-0.003 total time=  17.3s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005872 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3654\n",
      "[LightGBM] [Info] Number of data points in the train set: 219147, number of used features: 145\n",
      "[LightGBM] [Info] Start training from score 0.006051\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005673 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3701\n",
      "[LightGBM] [Info] Number of data points in the train set: 219146, number of used features: 166\n",
      "[LightGBM] [Info] Start training from score 0.006283\n",
      "[CV 3/3] END learning_rate=0.01, n_estimators=100, num_leaves=63;, score=-0.003 total time=  24.9s\n",
      "[CV 2/3] END learning_rate=0.01, n_estimators=200, num_leaves=31;, score=-0.002 total time=  25.3s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005655 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3714\n",
      "[LightGBM] [Info] Number of data points in the train set: 219147, number of used features: 167\n",
      "[LightGBM] [Info] Start training from score 0.006229\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005515 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3654\n",
      "[LightGBM] [Info] Number of data points in the train set: 219147, number of used features: 145\n",
      "[LightGBM] [Info] Start training from score 0.006051\n",
      "[CV 1/3] END learning_rate=0.01, n_estimators=100, num_leaves=63;, score=-0.002 total time=  28.3s\n",
      "[CV 3/3] END learning_rate=0.01, n_estimators=200, num_leaves=31;, score=-0.003 total time=  28.2s\n",
      "[CV 1/3] END learning_rate=0.01, n_estimators=200, num_leaves=31;, score=-0.002 total time=  28.9s\n",
      "[CV 2/3] END learning_rate=0.01, n_estimators=100, num_leaves=63;, score=-0.002 total time=  29.2s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006131 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3701\n",
      "[LightGBM] [Info] Number of data points in the train set: 219146, number of used features: 166\n",
      "[LightGBM] [Info] Start training from score 0.006283\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.019823 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3714\n",
      "[LightGBM] [Info] Number of data points in the train set: 219147, number of used features: 167\n",
      "[LightGBM] [Info] Start training from score 0.006229\n",
      "[CV 1/3] END learning_rate=0.1, n_estimators=100, num_leaves=31;, score=-0.002 total time=  12.7s\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010809 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3654\n",
      "[LightGBM] [Info] Number of data points in the train set: 219147, number of used features: 145\n",
      "[LightGBM] [Info] Start training from score 0.006051\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.021886 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3701\n",
      "[LightGBM] [Info] Number of data points in the train set: 219146, number of used features: 166\n",
      "[LightGBM] [Info] Start training from score 0.006283\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006039 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3714\n",
      "[LightGBM] [Info] Number of data points in the train set: 219147, number of used features: 167\n",
      "[LightGBM] [Info] Start training from score 0.006229\n",
      "[CV 2/3] END learning_rate=0.1, n_estimators=100, num_leaves=31;, score=-0.002 total time=  12.8s\n",
      "[CV 3/3] END learning_rate=0.1, n_estimators=100, num_leaves=31;, score=-0.003 total time=  12.8s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005263 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3654\n",
      "[LightGBM] [Info] Number of data points in the train set: 219147, number of used features: 145\n",
      "[LightGBM] [Info] Start training from score 0.006051\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006385 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3701\n",
      "[LightGBM] [Info] Number of data points in the train set: 219146, number of used features: 166\n",
      "[LightGBM] [Info] Start training from score 0.006283\n",
      "[CV 2/3] END learning_rate=0.1, n_estimators=100, num_leaves=63;, score=-0.002 total time=  19.8s\n",
      "[CV 3/3] END learning_rate=0.1, n_estimators=100, num_leaves=63;, score=-0.003 total time=  19.7s\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011359 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3714\n",
      "[LightGBM] [Info] Number of data points in the train set: 219147, number of used features: 167\n",
      "[LightGBM] [Info] Start training from score 0.006229\n",
      "[CV 1/3] END learning_rate=0.1, n_estimators=200, num_leaves=31;, score=-0.002 total time=  20.1s\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012159 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3654\n",
      "[LightGBM] [Info] Number of data points in the train set: 219147, number of used features: 145\n",
      "[LightGBM] [Info] Start training from score 0.006051\n",
      "[CV 1/3] END learning_rate=0.01, n_estimators=200, num_leaves=63;, score=-0.002 total time=  51.3s\n",
      "[CV 1/3] END learning_rate=0.1, n_estimators=100, num_leaves=63;, score=-0.002 total time=  23.5s\n",
      "[CV 2/3] END learning_rate=0.1, n_estimators=200, num_leaves=31;, score=-0.002 total time=  22.7s\n",
      "[CV 2/3] END learning_rate=0.01, n_estimators=200, num_leaves=63;, score=-0.002 total time=  39.0s\n",
      "[CV 3/3] END learning_rate=0.1, n_estimators=200, num_leaves=31;, score=-0.003 total time=  19.2s\n",
      "[CV 3/3] END learning_rate=0.01, n_estimators=200, num_leaves=63;, score=-0.003 total time=  40.7s\n",
      "[CV 1/3] END learning_rate=0.1, n_estimators=200, num_leaves=63;, score=-0.002 total time=  24.3s\n",
      "[CV 2/3] END learning_rate=0.1, n_estimators=200, num_leaves=63;, score=-0.002 total time=  14.8s\n",
      "[CV 3/3] END learning_rate=0.1, n_estimators=200, num_leaves=63;, score=-0.003 total time=  14.6s\n"
     ]
    }
   ],
   "source": [
    "lgbm_reg = lgb.LGBMRegressor()\n",
    "param_grid_lgbm_reg = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'learning_rate': [0.01, 0.1],\n",
    "    'num_leaves': [31, 63]\n",
    "}\n",
    "\n",
    "train_and_evaluate_model(\n",
    "    model=lgbm_reg,\n",
    "    param_grid=param_grid_lgbm_reg,\n",
    "    model_name=\"LightGBM Regressor\",\n",
    "    X_train=X_train_flat,\n",
    "    y_train=y_train_flat,\n",
    "    X_test=X_test_flat,\n",
    "    y_test=y_test_flat\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/threadpoolctl.py:1010: RuntimeWarning: \n",
      "Found Intel OpenMP ('libiomp') and LLVM OpenMP ('libomp') loaded at\n",
      "the same time. Both libraries are known to be incompatible and this\n",
      "can cause random crashes or deadlocks on Linux when loaded in the\n",
      "same Python program.\n",
      "Using threadpoolctl may cause crashes or deadlocks. For more\n",
      "information and possible workarounds, please see\n",
      "    https://github.com/joblib/threadpoolctl/blob/master/multiple_openmp.md\n",
      "\n",
      "  warnings.warn(msg, RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "K-Nearest Neighbors Regressor Run 1 parameters: {'n_neighbors': 3, 'weights': 'uniform'}\n",
      "K-Nearest Neighbors Regressor Run 1 Test MSE: 0.0740\n",
      "K-Nearest Neighbors Regressor Run 1 Test MAE: 0.2188\n",
      "K-Nearest Neighbors Regressor Run 1 Test R^2 Score: -0.2607\n",
      "K-Nearest Neighbors Regressor Run 1 Test MAPE: 115.5888\n",
      "K-Nearest Neighbors Regressor Run 1 Test RMSE: 0.2720\n",
      "K-Nearest Neighbors Regressor Run 1 Test sMAPE: 68.9993\n",
      "\n",
      "K-Nearest Neighbors Regressor Run 2 parameters: {'n_neighbors': 3, 'weights': 'distance'}\n",
      "K-Nearest Neighbors Regressor Run 2 Test MSE: 0.0771\n",
      "K-Nearest Neighbors Regressor Run 2 Test MAE: 0.2205\n",
      "K-Nearest Neighbors Regressor Run 2 Test R^2 Score: -0.3130\n",
      "K-Nearest Neighbors Regressor Run 2 Test MAPE: 115.8314\n",
      "K-Nearest Neighbors Regressor Run 2 Test RMSE: 0.2776\n",
      "K-Nearest Neighbors Regressor Run 2 Test sMAPE: 69.3117\n",
      "\n",
      "K-Nearest Neighbors Regressor Run 3 parameters: {'n_neighbors': 5, 'weights': 'uniform'}\n",
      "K-Nearest Neighbors Regressor Run 3 Test MSE: 0.0638\n",
      "K-Nearest Neighbors Regressor Run 3 Test MAE: 0.2105\n",
      "K-Nearest Neighbors Regressor Run 3 Test R^2 Score: -0.0870\n",
      "K-Nearest Neighbors Regressor Run 3 Test MAPE: 114.8410\n",
      "K-Nearest Neighbors Regressor Run 3 Test RMSE: 0.2526\n",
      "K-Nearest Neighbors Regressor Run 3 Test sMAPE: 66.8194\n",
      "\n",
      "K-Nearest Neighbors Regressor Run 4 parameters: {'n_neighbors': 5, 'weights': 'distance'}\n",
      "K-Nearest Neighbors Regressor Run 4 Test MSE: 0.0677\n",
      "K-Nearest Neighbors Regressor Run 4 Test MAE: 0.2122\n",
      "K-Nearest Neighbors Regressor Run 4 Test R^2 Score: -0.1541\n",
      "K-Nearest Neighbors Regressor Run 4 Test MAPE: 114.8444\n",
      "K-Nearest Neighbors Regressor Run 4 Test RMSE: 0.2602\n",
      "K-Nearest Neighbors Regressor Run 4 Test sMAPE: 67.0904\n",
      "\n",
      "K-Nearest Neighbors Regressor Run 5 parameters: {'n_neighbors': 7, 'weights': 'uniform'}\n",
      "K-Nearest Neighbors Regressor Run 5 Test MSE: 0.0608\n",
      "K-Nearest Neighbors Regressor Run 5 Test MAE: 0.2093\n",
      "K-Nearest Neighbors Regressor Run 5 Test R^2 Score: -0.0358\n",
      "K-Nearest Neighbors Regressor Run 5 Test MAPE: 114.2269\n",
      "K-Nearest Neighbors Regressor Run 5 Test RMSE: 0.2465\n",
      "K-Nearest Neighbors Regressor Run 5 Test sMAPE: 66.6251\n",
      "\n",
      "K-Nearest Neighbors Regressor Run 6 parameters: {'n_neighbors': 7, 'weights': 'distance'}\n",
      "K-Nearest Neighbors Regressor Run 6 Test MSE: 0.0649\n",
      "K-Nearest Neighbors Regressor Run 6 Test MAE: 0.2110\n",
      "K-Nearest Neighbors Regressor Run 6 Test R^2 Score: -0.1053\n",
      "K-Nearest Neighbors Regressor Run 6 Test MAPE: 114.9469\n",
      "K-Nearest Neighbors Regressor Run 6 Test RMSE: 0.2547\n",
      "K-Nearest Neighbors Regressor Run 6 Test sMAPE: 66.6601\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "knn_model = KNeighborsRegressor\n",
    "param_grid_knn = {\n",
    "    'n_neighbors': [3, 5, 7],\n",
    "    'weights': ['uniform', 'distance']\n",
    "}\n",
    "\n",
    "train_and_evaluate_model(\n",
    "    model_class=knn_model,\n",
    "    param_grid=param_grid_knn,\n",
    "    model_name=\"K-Nearest Neighbors Regressor\",\n",
    "    X_train=X_train_flat,\n",
    "    y_train=y_train_flat,\n",
    "    X_test=X_test_flat,\n",
    "    y_test=y_test_flat\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RaceRegressionModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=128):\n",
    "        super(RaceRegressionModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, 1)  # Output a score for each rider\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x should have shape (batch_size, num_features)\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class RaceRegressionDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)  # Shape: (num_samples, num_features)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)  # Shape: (num_samples,)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        X = self.X[idx]\n",
    "        y = self.y[idx]\n",
    "        return X, y\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = RaceRegressionDataset(X_train_flat, y_train_flat)\n",
    "test_dataset = RaceRegressionDataset(X_test_flat, y_test_flat)\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 256\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-30 17:40:20,519] A new study created in memory with name: no-name-7554d3a2-f650-4733-a0f8-a05d56f19011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-30 17:40:25,555] Trial 0 finished with value: 0.20807187259197235 and parameters: {'hidden_size': 128, 'learning_rate': 0.008768700064872995, 'weight_decay': 0.00017015278598835897, 'num_epochs': 24, 'batch_size': 64}. Best is trial 0 with value: 0.20807187259197235.\n",
      "[I 2024-11-30 17:40:28,673] Trial 1 finished with value: 0.20251713693141937 and parameters: {'hidden_size': 128, 'learning_rate': 0.004842895857136095, 'weight_decay': 0.0007043910310181157, 'num_epochs': 14, 'batch_size': 256}. Best is trial 1 with value: 0.20251713693141937.\n",
      "[I 2024-11-30 17:40:32,529] Trial 2 finished with value: 0.22294147312641144 and parameters: {'hidden_size': 64, 'learning_rate': 0.008527502498029825, 'weight_decay': 0.00018601829593207058, 'num_epochs': 29, 'batch_size': 256}. Best is trial 1 with value: 0.20251713693141937.\n",
      "[I 2024-11-30 17:40:35,902] Trial 3 finished with value: 0.21592111885547638 and parameters: {'hidden_size': 128, 'learning_rate': 0.0011029834025981058, 'weight_decay': 1.0840615657933877e-05, 'num_epochs': 11, 'batch_size': 256}. Best is trial 1 with value: 0.20251713693141937.\n",
      "[I 2024-11-30 17:40:40,841] Trial 4 finished with value: 0.35056740045547485 and parameters: {'hidden_size': 128, 'learning_rate': 0.004660343751724532, 'weight_decay': 0.000803667796093238, 'num_epochs': 27, 'batch_size': 64}. Best is trial 1 with value: 0.20251713693141937.\n",
      "[I 2024-11-30 17:40:44,564] Trial 5 finished with value: 0.2226119488477707 and parameters: {'hidden_size': 256, 'learning_rate': 0.002700161702088843, 'weight_decay': 0.0009862866422586211, 'num_epochs': 10, 'batch_size': 64}. Best is trial 1 with value: 0.20251713693141937.\n",
      "[I 2024-11-30 17:40:49,380] Trial 6 finished with value: 0.20014537870883942 and parameters: {'hidden_size': 64, 'learning_rate': 0.004436067174709775, 'weight_decay': 0.0005194994559993829, 'num_epochs': 29, 'batch_size': 64}. Best is trial 6 with value: 0.20014537870883942.\n",
      "[I 2024-11-30 17:40:52,999] Trial 7 finished with value: 0.21395854651927948 and parameters: {'hidden_size': 64, 'learning_rate': 0.008858232386941773, 'weight_decay': 9.007012054299354e-05, 'num_epochs': 12, 'batch_size': 64}. Best is trial 6 with value: 0.20014537870883942.\n",
      "[I 2024-11-30 17:40:56,820] Trial 8 finished with value: 0.19805841147899628 and parameters: {'hidden_size': 128, 'learning_rate': 0.00955428434426838, 'weight_decay': 0.000886153158994072, 'num_epochs': 18, 'batch_size': 256}. Best is trial 8 with value: 0.19805841147899628.\n",
      "[I 2024-11-30 17:41:02,156] Trial 9 finished with value: 0.3444497883319855 and parameters: {'hidden_size': 128, 'learning_rate': 0.007137960585782484, 'weight_decay': 0.000195398896335, 'num_epochs': 29, 'batch_size': 64}. Best is trial 8 with value: 0.19805841147899628.\n",
      "[I 2024-11-30 17:41:06,669] Trial 10 finished with value: 0.23221004009246826 and parameters: {'hidden_size': 256, 'learning_rate': 0.006597954135069507, 'weight_decay': 0.0009916206872096242, 'num_epochs': 18, 'batch_size': 128}. Best is trial 8 with value: 0.19805841147899628.\n",
      "[I 2024-11-30 17:41:10,623] Trial 11 finished with value: 0.25319811701774597 and parameters: {'hidden_size': 64, 'learning_rate': 0.0029968848806286126, 'weight_decay': 0.000459661590055071, 'num_epochs': 20, 'batch_size': 128}. Best is trial 8 with value: 0.19805841147899628.\n",
      "[I 2024-11-30 17:41:14,559] Trial 12 finished with value: 0.2076466977596283 and parameters: {'hidden_size': 64, 'learning_rate': 0.009955033974398588, 'weight_decay': 0.0004640640585973332, 'num_epochs': 23, 'batch_size': 256}. Best is trial 8 with value: 0.19805841147899628.\n",
      "[I 2024-11-30 17:41:18,006] Trial 13 finished with value: 0.19903157651424408 and parameters: {'hidden_size': 64, 'learning_rate': 0.006389520904208191, 'weight_decay': 0.0006289141729782682, 'num_epochs': 16, 'batch_size': 256}. Best is trial 8 with value: 0.19805841147899628.\n",
      "[I 2024-11-30 17:41:22,522] Trial 14 finished with value: 0.20277947187423706 and parameters: {'hidden_size': 256, 'learning_rate': 0.0065193499966956555, 'weight_decay': 0.0007439044527670713, 'num_epochs': 16, 'batch_size': 256}. Best is trial 8 with value: 0.19805841147899628.\n",
      "[I 2024-11-30 17:41:26,093] Trial 15 finished with value: 0.20052377879619598 and parameters: {'hidden_size': 128, 'learning_rate': 0.009979162250522922, 'weight_decay': 0.0006157879326640429, 'num_epochs': 16, 'batch_size': 256}. Best is trial 8 with value: 0.19805841147899628.\n",
      "[I 2024-11-30 17:41:29,715] Trial 16 finished with value: 0.21850337088108063 and parameters: {'hidden_size': 64, 'learning_rate': 0.0075357222377891095, 'weight_decay': 0.0008650699250429005, 'num_epochs': 21, 'batch_size': 256}. Best is trial 8 with value: 0.19805841147899628.\n",
      "[I 2024-11-30 17:41:33,599] Trial 17 finished with value: 0.20628370344638824 and parameters: {'hidden_size': 128, 'learning_rate': 0.005690467178263363, 'weight_decay': 0.00035474635939220374, 'num_epochs': 18, 'batch_size': 256}. Best is trial 8 with value: 0.19805841147899628.\n",
      "[I 2024-11-30 17:41:36,919] Trial 18 finished with value: 0.20218691229820251 and parameters: {'hidden_size': 64, 'learning_rate': 0.003452470617364857, 'weight_decay': 0.0008827457385494814, 'num_epochs': 14, 'batch_size': 128}. Best is trial 8 with value: 0.19805841147899628.\n",
      "[I 2024-11-30 17:41:40,970] Trial 19 finished with value: 0.20788460969924927 and parameters: {'hidden_size': 256, 'learning_rate': 0.0012117815803470468, 'weight_decay': 0.0005945942540407651, 'num_epochs': 18, 'batch_size': 256}. Best is trial 8 with value: 0.19805841147899628.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'hidden_size': 128, 'learning_rate': 0.00955428434426838, 'weight_decay': 0.000886153158994072, 'num_epochs': 18, 'batch_size': 256}\n",
      "Best sMAPE: 0.19805841147899628\n"
     ]
    }
   ],
   "source": [
    "# Define the objective function for Optuna\n",
    "def objective(trial):\n",
    "    # Hyperparameter suggestions\n",
    "    hidden_size = trial.suggest_categorical('hidden_size', [64, 128, 256])\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-2)\n",
    "    weight_decay = trial.suggest_float('weight_decay', 1e-6, 1e-3)\n",
    "    num_epochs = trial.suggest_int('num_epochs', 10, 30)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [64, 128, 256])\n",
    "\n",
    "    # Create data loaders with the suggested batch size\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    input_size = X_train_flat.shape[1]\n",
    "\n",
    "    # Initialize model, loss function, and optimizer\n",
    "    model = RaceRegressionModel(input_size, hidden_size).to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "    # Start MLflow run\n",
    "    with mlflow.start_run(run_name=f\"Neural Network MAE Run {trial.number}\"):\n",
    "        mlflow.log_params({\n",
    "            'model_class': 'RaceRegressionModel',\n",
    "            'hidden_size': hidden_size,\n",
    "            'learning_rate': learning_rate,\n",
    "            'weight_decay': weight_decay,\n",
    "            'num_epochs': num_epochs,\n",
    "            'batch_size': batch_size\n",
    "        })\n",
    "\n",
    "        # Training loop\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            total_loss = 0\n",
    "            for X_batch, y_batch in train_loader:\n",
    "                X_batch = X_batch.to(device)\n",
    "                y_batch = y_batch.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(X_batch)\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                total_loss += loss.item() * X_batch.size(0)\n",
    "\n",
    "            average_loss = total_loss / len(train_loader.dataset)\n",
    "            mlflow.log_metric(\"train_loss\", average_loss, step=epoch)\n",
    "\n",
    "        # Evaluation on test set\n",
    "        model.eval()\n",
    "        y_true_list = []\n",
    "        y_pred_list = []\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in test_loader:\n",
    "                X_batch = X_batch.to(device)\n",
    "                y_batch = y_batch.to(device)\n",
    "\n",
    "                outputs = model(X_batch)\n",
    "                y_true_list.extend(y_batch.cpu().numpy())\n",
    "                y_pred_list.extend(outputs.cpu().numpy())\n",
    "\n",
    "        y_true_array = np.array(y_true_list)\n",
    "        y_pred_array = np.array(y_pred_list)\n",
    "\n",
    "        # Compute evaluation metrics\n",
    "        mse = mean_squared_error(y_true_array, y_pred_array)\n",
    "        mae = mean_absolute_error(y_true_array, y_pred_array)\n",
    "        rmse = np.sqrt(mse)\n",
    "        r2 = r2_score(y_true_array, y_pred_array)\n",
    "        mape = mean_absolute_percentage_error(y_true_array, y_pred_array)\n",
    "        smape = symmetric_mean_absolute_percentage_error(y_true_array, y_pred_array)\n",
    "\n",
    "        # Log metrics\n",
    "        mlflow.log_metrics({\n",
    "            'test_mse': mse,\n",
    "            'test_mae': mae,\n",
    "            'test_rmse': rmse,\n",
    "            'test_r2': r2,\n",
    "            'test_mape': mape,\n",
    "            'test_smape': smape\n",
    "        })\n",
    "\n",
    "        # Log the model\n",
    "        input_example = X_train_flat[:5].astype(np.float32)\n",
    "        input_example_tensor = torch.tensor(input_example, dtype=torch.float32).to(device)\n",
    "        signature = infer_signature(\n",
    "            input_example,\n",
    "            model(input_example_tensor).cpu().detach().numpy()\n",
    "        )\n",
    "        mlflow.pytorch.log_model(\n",
    "            pytorch_model=model,\n",
    "            artifact_path=\"model\",\n",
    "            input_example=input_example,\n",
    "            signature=signature\n",
    "        )\n",
    "\n",
    "    # Return the metric to optimize\n",
    "    return mae\n",
    "\n",
    "# Create an Optuna study and optimize\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=20)\n",
    "\n",
    "# Print best hyperparameters\n",
    "print(\"Best hyperparameters:\", study.best_params)\n",
    "print(\"Best sMAPE:\", study.best_value)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
